\section{Lecture 1: Introduction to Statistical Learning Theory}
\subsection{Topic 1: Statistical Learning Theory}
\subsubsection{Learning Objectives} % This is new.
\begin{enumerate}
\item Identify the input, action, and outcome spaces for a given machine learning problem.
\item Provide an example for which the action space and outcome spaces are the same and one for which they are different.
\item Explain the relationships between the decision function, the loss function, the input space, the action space, and the outcome space.
\item Define the risk of a decision function and a Bayes decision function.
\item Provide example decision problems for which the Bayes risk is 0 and the Bayes risk is nonzero.
\item Know the Bayes decision functions for square loss and multiclass 0/1 loss.
\item Define the empirical risk for a decision function and the empirical risk minimizer.
\item Explain what a hypothesis space is, and how it can be used with constrained empirical risk minimization to control overfitting.
\end{enumerate}
\subsubsection{Concept Check Questions}
\begin{enumerate}
\item Suppose $\AA=\YY=\RR$ and $\XX$ is some other set.  Furthermore, assume
  $\Pr_{\XX\times \YY}$ is a discrete joint distribution.
  Compute a Bayes decision function when
  the loss function $\ell:\AA\times\YY\to\RR$ is
  given by
  $$\ell(a,y) = \Ind(a\neq y),$$
  the $0-1$ loss.
\begin{solution}
\item[]\Sol The Bayes decision function $f^*$ satisfies
  $$f^* = \argmin_f R(f) = \argmin_f \E[\Ind(f(X)\neq Y)] = \argmin_f
  \Pr(f(X)\neq Y),$$
  where $(X,Y)\sim \Pr_{\XX\times\YY}$.
  Let
  $$f_1(x) = \argmax_y \Pr(Y=y\mid X=x),$$
  the maximum a posteriori estimate of $Y$.
  If there is a tie, we choose any of the maximizers.  If $f_2$ is
  another decision function we have
  $$\begin{array}{rcll}
    \Pr(f_1(X)\neq Y)
    & = & \sum_x \Pr(f_1(x)\neq Y|X=x)\Pr(X=x)\\
    & = & \sum_x (1-\Pr(f_1(x)=Y|X=x))\Pr(X=x)\\
    & \leq & \sum_x (1-\Pr(f_2(x)=Y|X=x))\Pr(X=x) &\text{(Defn of
      $f_1$)}\\
    & = & \sum_x \Pr(f_2(x)\neq Y|X=x)\Pr(X=x)\\
    & = & \Pr(f_2(X)\neq Y).
  \end{array}$$
  Thus $f^*=f_1$.
\end{solution}
\item ($\star$) Suppose $\AA=\YY=\RR$, $\XX$ is some other set,
  and $\ell:\AA\times\YY\to\RR$ is given by
  $\ell(a,y)= (a-y)^2$, the square error loss.  What is the Bayes risk
  and how does it compare with the variance of $Y$?
\begin{solution}
\item[]\Sol From Homework~1 we know that the Bayes decision function
  is given by $f^*(x)=\E[Y|X=x]$.  Thus the Bayes risk is given by
  $$\E[(f^*(X)-Y)^2]=\E[(\E[Y|X]-Y)^2] = \E[\E[(\E[Y|X]-Y)^2|X]] = \E[\Var(Y|X)],$$
  where we applied the law of iterated expectations.
  The law of total variance states that
  $$\Var(Y) = \E[\Var(Y|X)] + \Var[\E(Y|X)].$$
  This proves the Bayes risk satisfies
  $$\E[\Var(Y|X)] = \Var(Y)-\Var[\E(Y|X)] \leq \Var(Y).$$
  Recall from Homework~1 that $\Var(Y)$ is the Bayes risk when we
  estimate $Y$ without any input $X$.  This shows that using $X$ in
  our estimation reduces the Bayes risk, and that the improvement is
  measured by $\Var[\E(Y|X)]$.  As a sanity check, note that if $X,Y$
  are independent then $\E(Y|X)=\E(Y)$ so $\Var[\E(Y|X)]=0$.  If $X=Y$
  then $\E(Y|X)=Y$ and $\Var[\E(Y|X)]=\Var(Y)$.

  The prominent role of variance in our analysis above is due to the
  fact that we are using the square loss.
\end{solution}
\item Let $\XX=\{1,\ldots,10\}$, let $\YY=\{1,\ldots,10\}$, and let
  $A=\YY$. Suppose the data generating distribution, $\PP$, has
  marginal $X\sim \Unif\{1,\ldots,10\}$ and conditional
  distribution $Y|X=x\sim \Unif\{1,\ldots,x\}$.  For each loss
  function below give a Bayes decision function.
  \begin{enumerate}
  \item $\ell(a,y) = (a-y)^2$,
  \item $\ell(a,y) = |a-y|$,
  \item $\ell(a,y) = \Ind(a\neq y)$.
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item From Homework~1 we know that $f^*(x)=\E[Y|X=x]=(x+1)/2$.
  \item From Homework~1, we know that $f^*(x)$ is the conditional
    median of $Y$ given $X=x$.  If $x$ is odd, then $f^*(x)=(x+1)/2$.
    If $x$ is even, then we can choose any value in the interval
    $$\left[\left\lfloor \frac{x+1}{2}\right\rfloor,\left\lceil \frac{x+1}{2}\right\rceil\right].$$
  \item From question~1 above, we know that $f^*(x)=\argmax_y
    \Pr(Y=y|X=x)$.  Thus we can choose any integer between $1$ and $x$,
    inclusive, for $f^*(x)$.
  \end{enumerate}
\end{solution}
\item Show that the empirical risk is an unbiased and
  consistent estimator of the Bayes risk. You may assume the Bayes
  risk is finite.
\begin{solution}
\item[]\Sol We assume a given loss function $\ell$ and an
  i.i.d.~sample $(x_1,y_i),\ldots,(x_n,y_n)$.
  To show it is unbiased, note that
  $$\begin{Array}{rcll}
    \E[\hat{R}_n(f)]
    & = & \E\left[\frac{1}{n}\sum_{i=1}^n \ell(f(x_i),y_i)\right]\\
    & = & \frac{1}{n}\sum_{i=1}^n\E[\ell(f(x_i),y_i)] &
    \text{(Linearity of $\E$)}\\
    & = & \E[\ell(f(x_1),y_1)] & \text{(i.i.d.)}\\
    & = & R(f).
  \end{Array}$$
  For consistency, we must show that as $n\to\infty$ we have
  $\hat{R}_n(f)\to R(f)$ with probability~1.  Letting
  $z_i=\ell(f(x_i),y_i)$, we see that the $z_i$ are i.i.d.~with finite
  mean.  Thus consistency follows by applying the strong law of large numbers.
\end{solution}
\item Let $\XX=[0,1]$ and $\YY=\AA=\RR$.  Suppose you receive the
  $(x,y)$ data points $(0,5)$, $(.2,3)$, $(.37,4.2)$, $(.9,3)$, $(1,5)$.  Throughout
  assume we are using the $0-1$ loss.
  \begin{enumerate}
  \item Suppose we restrict our decision functions to the hypothesis
    space $\FF_1$ of constant functions.  Give a decision function that
    minimizes the empirical risk over $\FF_1$ and the corresponding
    empirical risk.  Is the empirical risk minimizing function unique?
  \item Suppose we restrict our decision functions to the hypothesis
    space $\FF_2$ of piecewise-constant functions with at most 1
    discontinuity.  Give a decision function that
    minimizes the empirical risk over $\FF_2$ and the corresponding
    empirical risk.  Is the empirical risk minimizing function unique?
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item We can let $\hat{f}(x)=5$ or $\hat{f}(x)=3$ and obtain the
    minimal empirical risk of $3/5$.  Thus the empirical risk
    minimizer is not unique.
  \item One solution is to let $\hat{f}(x)=5$ for $x\in[0,.1]$ and
    $\hat{f}(x)=3$ for $x\in(.1,1]$ giving an empirical risk of
    $2/5$.  There are uncountably many empirical risk minimizers, so
    again we do not have uniqueness.
  \end{enumerate}
\end{solution}
\item $(\star)$ Let $\XX=[-10,10]$, $\YY=\AA=\RR$ and suppose the data generating
  distribution has marginal distribution $X\sim\Unif[-10,10]$ and
  conditional distribution $Y|X=x\sim
  \Norm(a+bx,1)$ for some fixed $a,b\in\RR$.  Suppose you are also
  given the following data points: $(0,1)$, $(0,2)$, $(1,3)$,
  $(2.5,3.1)$, $(-4,-2.1)$.
  \begin{enumerate}
  \item Assuming the $0-1$ loss, what is the Bayes risk?
  \item Assuming the square error loss $\ell(a,y)=(a-y)^2$, what is the Bayes risk?
  \item Using the full hypothesis space of all (measurable) functions,
    what is the minimum achievable empirical risk for the square error loss?
  \item Using the hypothesis space of all affine functions (i.e., of
    the form $f(x)=cx+d$ for some $c,d\in\RR$),
    what is the minimum achievable empirical risk for the square error loss?
  \item Using the hypothesis space of all quadratic functions (i.e., of
    the form $f(x)=cx^2+dx+e$ for some $c,d,e\in\RR$),
    what is the minimum achievable empirical risk for the square error
    loss?
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item For any decision function $f$ the risk is given by
    $$\E[\Ind(f(X)\neq Y)] = \Pr(f(X)\neq Y) = 1 - \Pr(f(X)=Y)=1.$$
    To see this note that
    $$\Pr(f(X)=Y) =
    \frac{1}{20\sqrt{2\pi}}\int_{-10}^{10}\int_{-\infty}^\infty
    \Ind(f(x)=y)e^{-(y-a-bx)^2/2}\,dy\,dx
    = \frac{1}{20\sqrt{2\pi}}\int_{-10}^{10} 0\,dx = 0.$$
    Thus every decision function is a Bayes decision function, and the
    Bayes risk is~$1$.
  \item By problem~2 above we know the Bayes risk is given by
    $$\E[\Var(Y|X)] = \E[1]=1,$$
    since $\Var(Y|X=x)=1$.
  \item We choose $\hat{f}$ such that
    $$\hat{f}(0)=1.5, \hat{f}(1)=3, \hat{f}(2.5)=3.1,
    \hat{f}(-4)=2.1,$$
    and $\hat{f}(x)=0$ otherwise.  Then we achieve the minimum
    empirical risk of $1/10$.
  \item Letting
    $$A=\begin{pmatrix}1&0\\1&0\\1&1\\1&2.5\\1&-4\end{pmatrix},\quad y =
    \begin{pmatrix}1\\2\\3\\3.1\\-2.1\end{pmatrix}$$
    we obtain (using a computer)
    $$\hat{w}=\begin{pmatrix}\hat{d}\\\hat{c}\end{pmatrix} = (A^TA)^{-1}A^Ty
    = \begin{pmatrix}1.4856\\0.8556\end{pmatrix}.$$
    This gives
    $$\hat{R}_5(\hat{f}) = \frac{1}{5}\|A\hat{w}-y\|_2^2 = 0.2473.$$
    [Aside: In general, to solve systems like the one above on a computer you shouldn't actually invert
      the matrix $A^TA$, but use something like \lstinline!w=A\y! in
      Matlab which performs a QR factorization of $A$.]
  \item Letting
    $$A=\begin{pmatrix}1&0&0\\1&0&0\\1&1&1\\1&2.5&6.25\\1&-4&16\end{pmatrix},\quad y =
    \begin{pmatrix}1\\2\\3\\3.1\\-2.1\end{pmatrix}$$
    we obtain (using a computer)
    $$\hat{w}=\begin{pmatrix}\hat{e}\\\hat{d}\\\hat{c}\end{pmatrix} = (A^TA)^{-1}A^Ty
    = \begin{pmatrix}1.7175\\0.7545\\-0.0521\end{pmatrix}.$$
    This gives
    $$\hat{R}_5(\hat{f}) = \frac{1}{5}\|A\hat{w}-y\|_2^2 = 0.1928.$$
  \end{enumerate}
\end{solution}
\end{enumerate}

