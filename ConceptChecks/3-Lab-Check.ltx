\section{Week 3 Lab: Concept Check Exercises}
\subsection{Convexity}
\subsubsection{Optional Learning Objectives}
Convex optimization and Lagrangian duality will not covered on the midterm exam, so in some sense these objectives are optional.
\begin{itemize}
\item Define a convex set, a convex function, and a strictly convex function. (Don't forget that the domain of a convex function must be a convex set!)
\item For an optimization problem, define the terms feasible set, feasible point, active constraint, optimal value, and optimal point.
\item Give the form for a general inequality-constrained optimization problem (there are many ways to do this, but our convention is to have inequality constraints of the form $f_i(x)\leq0$).
\item Define the Lagrangian for this optimization problem, and explain how the Lagrangian encodes all the information in the original optimization problem.
\item Write the primal and dual optimization problem in terms of the Lagrangian.
\end{itemize}
\subsubsection{Convexity Concept Check Problems}
\begin{enumerate}
\item If $A,B\subseteq\RR^n$ are convex, then $A\cap B$ is convex.
\begin{solution}
\item[]\Sol Let $x,y\in A\cap B$ and $t\in(0,1)$.  Since $A,B$ are
  convex, we have
  $$(1-t)x+ty\in A\quad\text{and}\quad (1-t)x+ty \in B.$$
  Thus $(1-t)x+ty \in A\cap B$.
\end{solution}
\item Let $f,g:\RR^n\to\RR$ be convex.  Show that $af+bg$ is convex if
  $a,b\geq0$.
\begin{solution}
\item[]\Sol Let $x,y\in\RR^n$ and $\theta\in(0,1)$.  Then
  $$\begin{array}{rcl}
  (af+bg)((1-\theta)x+\theta y)
  & = & af((1-\theta)x+\theta y)+bg((1-\theta)x+\theta y)\\
  & \leq & a[(1-\theta)f(x) + \theta f(y)] + b[(1-\theta)g(x) + \theta
    g(y)]\\
  & = & (1-\theta)(af+bg)(x) + \theta(af+bg)(y).
  \end{array}$$
\end{solution}
\item Let $f:\RR^n\to\RR$ be convex and differentiable.  Prove that if
  $\nabla f(x)=0$ then $x$ is a global minimizer.
\begin{solution}
\item[]\Sol Suppose $\nabla f(x)=0$.  The gradient (or first-order)
  characterization of convexity says
  $$f(y) \geq f(x) + \nabla f(x)^T(y-x)$$
  for all $y$.  If $\nabla f(x)=0$ then this says $f(y)\geq f(x)$ for
  all $x$.
\end{solution}
\item Prove that if $f:\RR^n\to\RR$ is strictly convex and $x$ is a global
  minimizer, then it is the unique global minimizer.
\begin{solution}
\item[]\Sol Suppose $y$ is also a global minimizer with $y\neq x$.  Then
  $$f( (y+x)/2 ) < f(y)/2 + f(x)/2 = f(x)$$
  contradicting the fact that $f(x)$ was a global minimizer.
\end{solution}
\item Prove that any affine function $f:\RR^n\to\RR$ is both convex
  and concave.
\begin{solution}
\item[]\Sol Recall that $f$ has the form $f(x)=w^Tx+b$ where
  $w\in\RR^n$ and $b\in\RR$.  Then, for $x,y\in\RR^n$ and $\theta\in(0,1)$,
  $$f((1-\theta)x+\theta y) = w^T((1-\theta)x+\theta y)+b =
  (1-\theta)(w^Tx+b) + \theta(w^Ty+b) = (1-\theta)f(x) + \theta
  f(y).$$
  This shows $f$ is convex.  But the same holds if we replace $w$ with
  $-w$ and $b$ with $-b$.  Hence $f$ is also concave.
\end{solution}
\item Let $f:\RR^n\to\RR$ be convex and let $g:\RR^m\to\RR^n$ be
  affine.  Then $f\circ g$ is convex.
\begin{solution}
\item[]\Sol Write $g(x)=Ax+b$ where $A\in\RR^{n\times m}$ and
  $b\in\RR^n$.  For $x,y\in\RR^m$ and $t\in(0,1)$ we have
  $$\begin{array}{rcl}
  f(g((1-t)x + ty))
  & = & f((1-t)(Ax+b) + t(Ay + b))\\
  & \leq & (1-t)f(Ax+b) + tf(Ay+b)\\
  & = & (1-t) f(g(x)) + tf(g(y)).
  \end{array}$$
\end{solution}
\item $(\star\star)$
  \begin{enumerate}
  \item Let $f:\RR\to\RR$ be convex.  Show that $f$ has one-sided left and
    right derivatives at every point.
  \item Let $f:\RR^n\to\RR$ be convex.  Show that $f$ has one-sided
    directional derivatives at every point.
  \item Let $f:\RR^n\to\RR$ be convex.  Show that if $x$ is not a
    minimizer of $f$ then $f$ has a descent direction at $x$ (i.e., a
    direction whose corresponding one-sided directional derivative is negative).
  \end{enumerate}
\begin{solution}
\item[]\Sol We first prove the following lemma.
  \begin{lem}\label{lem:secant}
    If $f:\RR\to\RR$ is convex and $x < y < z$ then
    $$\frac{f(y)-f(x)}{y-x} \leq \frac{f(z)-f(x)}{z-x}.$$
  \end{lem}
  \begin{proof}
    Let $t\in(0,1)$ satisfy $(1-t)x+tz=y$.  By convexity we have
    $$f(y) = f( (1-t)x+tz ) \leq (1-t)f(x) + tf(z)$$
    giving
    $$\frac{f(y)-f(x)}{y-x} \leq \frac{(1-t)f(x) +
      tf(z)-f(x)}{(1-t)x+tz-x} = \frac{t(f(z)-f(x))}{t(z-x)}
    =\frac{f(z)-f(x)}{z-x}.$$
  \end{proof}
  \begin{enumerate}
  \item For the right derivative, we will show
    $$\lim_{y\downto x} \frac{f(y)-f(x)}{y-x} =
    \inf_{y>x}\frac{f(y)-f(x)}{y-x}=:L.$$
    Fix $\epsilon>0$ and choose $y'>x$ so that
    $$\frac{f(y')-f(x)}{y'-x} < L+\epsilon.$$
    Letting $\delta = y'-x$, the lemma shows that
    $$\frac{f(y)-f(x)}{y-x} < L+\epsilon$$
    for any $y < x+\delta$ proving the limit exists.

    For the left derivative, we could repeat the above, or note that
    $g(t) = 2x-t$ is affine, so $f\circ g$ is convex.  By the above
    $$\lim_{y\downto x} \frac{f(g(y))-f(g(x))}{y-x}
    = \lim_{y\downto x} \frac{f(2x-y)-f(x)}{y-x}
    = \lim_{h\downto 0} \frac{f(x-h)-f(x)}{h}$$
    exists, where $h=y-x$.  This proves the left derivative exists as
    well.
  \item Fix $x,v\in\RR^n$ and let $g:\RR\to\RR^n$ be defined by
    $g(t)=x+tv$.  Then $f\circ g$ is convex, and thus the previous
    part applies.  But the right derivative of $g$ at $0$ is the
    one-sided directional derivative of $f$ at $x$ in the direction $v$:
    $$\lim_{h\downto 0}\frac{f(g(h))-f(g(0))}{h} = \lim_{h\downto
      0}\frac{f(x+hv)-f(x)}{h}.$$
  \item Let $y$ be a minimizer of $f$ and let $g(t) = x+t(y-x)$.
    By the arguments in the first part above, the value
    $$\frac{f(g(1))-f(g(0))}{1-0} = f(y)-f(x) < 0$$
    is an upper bound on the right derivative of $g$ at $0$.  But this
    is a directional derivative, by the argument in the second part above.
  \end{enumerate}
\end{solution}
\end{enumerate}
\subsection{Convex Optimization Problems}
\begin{enumerate}
\item Suppose there are $mn$ people forming $m$ rows with $n$
  columns.  Let $a$ denote the height of the tallest person taken from
  the shortest people in each column.
  Let $b$ denote the height of the shortest person taken from the
  tallest people in each row. What is the relationship between $a$
  and $b$?
\begin{solution}
\item[]\Sol Let $H_{ij}$ denote the height of the person in row $i$
  and column $j$.  Then
  $$a = \max_j \min_i H_{ij} \leq \min_i\max_j H_{ij} = b,$$
  by the max-min inequality.
\end{solution}
\item Let $x_1,\ldots,x_n\in\RR^d$ be given data.  You want to find
  the center and radius of the smallest sphere that encloses all of
  the points.  Express this problem as a convex optimization problem.
\begin{solution}
\item[]\Sol
  $$\begin{array}{ll}
  \text{minimize}_{r,c} & r \\
  \text{subject to} & \|x_i-c\|_2 \leq r\quad\text{for $i=1,\ldots,n$.}
  \end{array}$$
  This problem is convex since norms are convex, so $f_i(c) =
  \|x_i-c\|_2$ is convex (composition of convex with affine).
\end{solution}
\item Suppose $x_1,\ldots,x_n\in\RR^d$ and
  $y_1,\ldots,y_n\in\{-1,1\}$.  Here we look at $y_i$ as the label of
  $x_i$.  We say the data points are linearly separable if there is a
  vector $v\in\RR^d$ and $a\in\RR$ such that $v^Tx_i>a$ when $y_i=1$
  and $v^Tx_i<a$ for $y_i=-1$.  Give a method for determining if the
  given data points are linearly separable.
\begin{solution}
\item[]\Sol Solve the hard-margin SVM problem
  $$\begin{array}{ll}
  \text{minimize}_{w,b} & \|w\|_2^2 \\
  \text{subject to} & y_i(w^Tx_i+b) \geq 1\quad\text{for all $i=1,\ldots,n$.}
  \end{array}$$
  If the resulting problem is feasible, then the data is linearly separable.
\end{solution}
\item Consider the Ivanov form of ridge regression:
  $$\begin{array}{ll}
  \text{minimize} & \|Ax-y\|_2^2 \\
  \text{subject to} & \|x\|_2^2 \leq r^2,
  \end{array}$$
  where $r>0$, $y\in\RR^m$ and $A\in\RR^{m\times n}$ are fixed.
  \begin{enumerate}
  \item What is the Lagrangian?
  \item What do you get when you take the supremum of the Lagrangian
    over the feasible values for the dual variables?
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item $L(x,\lambda) = \|Ax-y\|_2^2 + \lambda(\|x\|_2^2-r^2)$.  Note that
    this is a shifted version of the Tikhonov objective.
  \item
    $$\sup_{\lambda\succeq 0} L(x,\lambda) = \left\{\begin{array}{ll}
    +\infty & \text{if $\|x\|_2^2>r^2$,}\\
    \|Ax-y\|_2^2 & \text{otherwise.}\end{array}\right.$$
    Note that the original Ivanov minimization is then just
    $$\inf_x\sup_{\lambda\succeq 0}L(x,\lambda).$$
  \end{enumerate}
\end{solution}
\end{enumerate}

\subsection{Subgradients}
\begin{enumerate}
\item ($\star$) If $f:\RR^n\to\RR$ is convex and differentiable at $x$, the
  $\partial f(x) = \{\nabla f(x)\}$.
\begin{solution}
\item[]\Sol By the gradient (first-order) conditions for convexity, we
  know that $\nabla f(x) \in\partial f(x)$.  Next suppose $g\in
  \partial f(x)$.  This means that for all $v\in\RR^n$ and $h\in\RR$ we have
  $$f(x+hv) \geq f(x) + hg^Tv \implies \frac{f(x+hv)-f(x)}{h} \geq g^Tv.$$
  Using $-h$ in place of $h$ gives
  $$f(x-hv) \geq f(x) - hg^Tv \implies g^Tv \geq
  \frac{f(x-hv)-f(x)}{-h}.$$
  Taking limits as $h\to0$ gives
  $$\nabla f(x)^Tv \geq g^Tv \geq \nabla f(x)^Tv.$$
  Thus all terms are equal.  Subtracting gives
  $$(\nabla f(x) - g)^Tv = 0,$$
  which holds for all $v\in\RR^n$.  Letting $v=\nabla f(x) - g$ proves
  $$\|\nabla f(x) - g\|_2^2 = 0$$
  giving the result.
\end{solution}
\item Fix $f:\RR^n\to\RR$ and $x\in\RR^n$. Then the subdifferential $\partial f(x)$ is a
  convex set.
\begin{solution}
\item[]\Sol Let $g_1,g_2\in \partial f(x)$ and $t\in(0,1)$.  We must
  show $(1-t)g_1+tg_2$ is a subgradient.  Note that, for any
  $y\in\RR^n$, we have
  $$\begin{array}{rcl}
    f(x) + ((1-t)g_1+tg_2)^T(y-x) 
    & = & (1-t)(f(x)+g_1^T(y-x)) + t(f(x)+g_2^T(y-x)) \\
    & \leq & (1-t)f(y) + tf(y)\\
    & = & f(y).
  \end{array}$$
\end{solution}
\item 
  \begin{enumerate}
  \item True or False: A subgradient of $f:\RR^n\to\RR$ at $x$ is normal to a
    hyperplane that globally understimates the graph of $f$.
  \item True or False: If $g\in\partial f(x)$ then $-g$ is a descent
    direction of $f$.
  \item True or False: For $f:\RR\to\RR$, if $1,-1\in \partial f(x)$
    then $x$ is a global minimizer of $f$.
  \item True or False: Let $f:\RR^n\to\RR$ and let $g\in \partial
    f(x)$.  Then $\alpha g\in\partial f(x)$ for all $\alpha\in[0,1]$.
  \item True or False: If the sublevel sets of a function are convex,
    then the function is convex.
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item False. The underestimating hyperplane is a subset of
    $\RR^{n+1}$ but a subgradient is an element of $\RR^n$.
  \item False. In lab we considered $f(x_1,x_2)=|x_1|+2|x_2|$ and noted
    that $(1,-2)\in \partial f(3,0)$ but $(-1,2)$ is not a descent
    direction.
  \item True. The subdifferential of $f$ at $x$ is convex, and thus contains $0$.
    If $0$ is a subgradient of $f$ at $x$, then $x$ is a global
    minimizer.
  \item False.  Suppose $f:\RR\to\RR$ is defined by $f(x)=x^2$.  Then
    $\partial f(1) = \{2\}$, and thus doesn't contain $2\alpha$ for
    $\alpha\in[0,1)$.
  \item False.  A counterexample is $f(x)=-e^{-x^2}$.  The converse is
    true though. Functions that have convex sublevel sets are called \textit{quasiconvex}.
  \end{enumerate}
\end{solution}
\item Let $f:\RR^2\to\RR$ be defined by $f(x_1,x_2) = |x_1| +2|x_2|$.
  Compute $\partial f(x_1,x_2)$ for each $x_1,x_2\in\RR^2$.
\begin{solution}
\item[]\Sol Write $f(x_1,x_2) = f_1(x_1,x_2) + f_2(x_1,x_2)$ where
  $f_1(x_1,x_2)=|x_1|$ and $f_2(x_1,x_2)=2|x_2|$.  When $x_1\neq 0$ we
  have $\partial f_1(x_1,x_2) = \{(\sgn(x_1),0)^T\}$ and when $x_1=0$ we
  have
  $$\partial f_1(x_1,x_2) = \{(b,0)^T\mid b\in[-1,1]\}.$$
  When $x_2\neq 0$ we
  have $\partial f_2(x_1,x_2) = \{(0,2\sgn(x_2))^T\}$ and when $x_2=0$ we
  have
  $$\partial f_2(x_1,x_2) = \{(0,c)^T\mid c\in[-2,2]\}.$$
  Combining we have
  $$\partial f(x_1,x_2) = \partial f_1(x_1,x_2) + \partial
  f_2(x_1,x_2),$$
  where we are summing sets.  Recall that if $A,B\subseteq\RR^n$ then
  $$A+B = \{a+b \mid a\in A,b\in B\}.$$
  This gives 4 cases:
  \begin{enumerate}
  \item If $x_1,x_2\neq 0$ this gives $\partial f(x_1,x_2) =
    \{(\sgn(x_1),2\sgn(x_2))^T\}$.  
  \item If $x_1=0$ and $x_2\neq 0$ we have $\partial f(x_1,x_2) =
    \{(b,2\sgn(x_2))^T\mid b\in[-1,1]\}$.  
  \item If $x_1\neq0$ and $x_2= 0$ we have $\partial f(x_1,x_2) =
    \{(\sgn(x_1),c)^T\mid c\in[-2,2]\}$.  
  \item If $x_1=0$ and $x_2=0$ we have $\partial f(x_1,x_2) =
    \{(b,c)^T\mid b\in[-1,1],c\in[-2,2]\}$.  
  \end{enumerate}
\end{solution}
\end{enumerate}
