#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Bayesian Regression 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
Julia Kempe & David S.
 Rosenberg 
\end_layout

\begin_layout Date
March 26, 2019
\end_layout

\begin_layout Institute
CDS, NYU
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Contents
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Enumerate
Still need to get rid of references to probability models.
\end_layout

\begin_layout Enumerate
It's a bit confusing about why here we don't assume a joint distribution
 on 
\begin_inset Formula $\cx\times\cy$
\end_inset

 and usually we do.
 We actually required the joint distribution to talk about generalization,
 and we still do now.
 To get the regularized ERM, we actually only needed data.
 
\end_layout

\begin_layout Enumerate
Could use a diagram connecting prior distribution to prior-predictive distributi
on and posterior to posterior predictive distribution
\end_layout

\begin_layout Enumerate
Should we change from 
\begin_inset Formula $p(\theta)$
\end_inset

 to 
\begin_inset Formula $\pi(\theta)$
\end_inset

 for prior?
\end_layout

\begin_layout Enumerate
Could integrate discussion of conditional probability models corresponding
 to hypothesis spaces of predictive distributions earlier into the recap?
\end_layout

\begin_layout Enumerate
Would be good to have a simple example or a prior/posterior predictive distribut
ion – the notion of averaging distributions may need justification
\end_layout

\begin_layout Enumerate
Can I replace my pictures from Bishop with those from the homework solutions?
\end_layout

\begin_layout Enumerate
Work out why the posterior mean is unbiased when prior variance goes to
 infinity.
 
\begin_inset Formula $\ex\left[\left(X^{T}X\right)^{-1}X^{T}y\right]=\left(X^{T}X\right)^{-1}X^{T}\ex\left[Xw+\eps\right]=\left(X^{T}X\right)^{-1}X^{T}Xw=w$
\end_inset

.
\end_layout

\begin_layout Enumerate
Generally speaking, this lecture needs a real shakeup – maybe more pictures
 and demonstrations of more of the concepts? prediction intervals?
\end_layout

\begin_layout Enumerate
Really need to clarify the conditional independence stuff...
 maybe I should leave 
\begin_inset Formula $x$
\end_inset

 out of the conditional? And then clarify the relationship to the 
\begin_inset Formula $\left(x,y\right)$
\end_inset

 iid setting of machine learning.
\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Recap: Conditional Probability Models
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Conditional Probability Modeling
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Input space
\series default
 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Outcome space
\series default
 
\begin_inset Formula $\cy$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action space
\series default
 
\begin_inset Formula $\ca=\left\{ p(y)\mid p\text{ is a probability distribution on }\cy\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Hypothesis space
\series default
 
\begin_inset Formula $\cf$
\end_inset

 contains prediction functions 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Prediction function
\series default
 
\begin_inset Formula $f\in\cf$
\end_inset

 takes input 
\begin_inset Formula $x\in\cx$
\end_inset

 and produces a 
\series bold
distribution
\series default
 on 
\begin_inset Formula $\cy$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We've been discussing 
\series bold
parametric families of conditional densities
\series default

\begin_inset Formula 
\[
\left\{ p(y\mid x,\theta):\theta\in\Theta\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
These are also hypothesis spaces for conditional probability modeling.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Examples?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Parametric Family of Conditional Densities
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A 
\series bold
parametric family of conditional densities 
\series default
is a set 
\begin_inset Formula 
\[
\left\{ p(y\mid x,\theta):\theta\in\Theta\right\} ,
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
where 
\begin_inset Formula $p(y\mid x,\theta)$
\end_inset

 is a density on 
\series bold
outcome space 
\series default

\begin_inset Formula $\cy$
\end_inset

 for each 
\begin_inset Formula $x$
\end_inset

 in 
\series bold
input space 
\begin_inset Formula $\cx$
\end_inset


\series default
, and
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta$
\end_inset

 is a 
\series bold
parameter
\series default
 in a [finite dimensional] 
\series bold
parameter space 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
This is the common starting point for a treatment of classical or Bayesian
 statistics.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Density vs Mass Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In this lecture, whenever we say 
\begin_inset Quotes eld
\end_inset

density
\begin_inset Quotes erd
\end_inset

, we could replace it with 
\begin_inset Quotes eld
\end_inset

mass function.
\begin_inset Quotes erd
\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Corresponding integrals would be replaced by summations.

\size scriptsize
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\size small
(In more advanced, measure-theoretic treatments, they are each considered
 densities w.r.t.
 different base measures.)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Data: Assumptions So Far in this Course
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Our usual setup is that 
\begin_inset Formula $(x,y)$
\end_inset

 pairs are drawn i.i.d.
 from 
\begin_inset Formula $\cp_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How have we used this assumption so far?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
ties validation performance to test performance
\end_layout

\begin_layout Itemize
ties test performance to performance on new data when deployed
\end_layout

\begin_layout Itemize
motivates empirical risk minimization
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The large majority of things we've learned about ridge/lasso/elastic-net
 regression, optimization, SVMs, and kernel methods are true for arbitrary
 training data sets 
\begin_inset Formula $\cd:\left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\in\cx\times\cy$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
i.e.
 
\begin_inset Formula $\cd$
\end_inset

 could be created by hand, by an adversary, or randomly.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We rely on the i.i.d.
 
\begin_inset Formula $\cp_{\cx\times\cy}$
\end_inset

 assumption when it comes to 
\series bold
generalization
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Data: Conditional Probability Modeling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To get generalization, we'll still need our usual i.i.d.
 
\begin_inset Formula $\cp_{\cx\times\cy}$
\end_inset

 assumption.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
For developing the model, we'll make some assumptions about the training
 data...
\end_layout

\begin_deeper
\begin_layout Itemize
In most of what we've done before, we had no assumptions on the training
 data.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
It's typical (and most general) to do everything 
\begin_inset Quotes eld
\end_inset

conditional on the 
\begin_inset Formula $x$
\end_inset

's
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
That means, we assume the 
\begin_inset Formula $x$
\end_inset

's are known
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In particular, we do not consider them random
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We don't care how they were generated (randomly, adversarially, chosen by
 hand) 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In other words, still no assumptions on 
\begin_inset Formula $x$
\end_inset

's.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Data: Conditional Probability Modeling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So we assume the 
\begin_inset Formula $x$
\end_inset

's are known.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We observe 
\begin_inset Formula $y_{i}$
\end_inset

 sampled randomly from 
\begin_inset Formula $p(y\mid x_{i},\theta)$
\end_inset

, for some unknown 
\begin_inset Formula $\theta\in\Theta$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We assume the outcomes 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

 are independent.
 
\end_layout

\begin_deeper
\begin_layout Itemize
But not i.i.d.
 – Why?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Each 
\begin_inset Formula $y_{i}$
\end_inset

 may be drawn from a different distribution, depending on 
\begin_inset Formula $x_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Data: 
\series default

\begin_inset Formula $\cd=\left((y_{1},\ldots,,y_{n})\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Notation
\series default
: 
\begin_inset Formula $y=\left(y_{1},\ldots,y_{n}\right)$
\end_inset

 and 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{n}\right)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Data: 
\series default

\begin_inset Formula $\cd=(y_{1},\ldots,,y_{n})$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
Data: 
\series default

\begin_inset Formula $\cd=\left((x_{1},y_{1}),\ldots,(x_{n},y_{n})\right)$
\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
Notation
\series default
: 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{n}\right)$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The probability density for our data 
\begin_inset Formula $\cd$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
p(\cd\mid x_{1},\ldots,x_{n},\theta) & = & \prod_{i=1}^{n}p(y_{i}\mid x_{i},\theta).
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For fixed 
\begin_inset Formula $\cd$
\end_inset

, the function 
\begin_inset Formula $\theta\mapsto p(\cd\mid x,\theta)$
\end_inset

 is the 
\series bold
likelihood function
\series default
:
\begin_inset Formula 
\[
L_{\cd}(\theta)=p(\cd\mid x,\theta),
\]

\end_inset

where 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{n}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Maximum Likelihood Estimator
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
maximum likelihood estimator (MLE)
\series default
 for 
\begin_inset Formula $\theta$
\end_inset

 in the family 
\begin_inset Formula $\left\{ p(y\mid x,\theta)\mid\theta\in\Theta\right\} $
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{\text{MLE}} & = & \argmax_{\theta\in\Theta}L_{\cd}(\theta).
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
MLE corresponds to ERM for the negative log- likelihood loss (discussed
 previously).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The corresponding prediction function is
\begin_inset Formula 
\[
\hat{f}(x)=p(y\mid x,\hat{\theta}_{\text{MLE}}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can think of this as a choice of a particular function from the hypothesis
 space
\begin_inset Formula 
\[
\cf=\left\{ p(y\mid x,\theta):\theta\in\Theta\right\} .
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Bayesian Conditional Probability Models
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian Conditional Models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset space \qquad{}
\end_inset

Outcome space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Two components to Bayesian conditional model:
\end_layout

\begin_deeper
\begin_layout Itemize
A 
\series bold
parametric family of conditional densities
\series default
: 
\begin_inset Formula 
\[
\left\{ p(y\mid x,\theta):\theta\in\Theta\right\} 
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
A 
\series bold
prior distribution
\series default
 
\begin_inset Formula $p(\theta)$
\end_inset

 on 
\begin_inset Formula $\theta\in\Theta$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Pause

\end_layout

\begin_layout Itemize
For simplicity, can leave 
\begin_inset Formula $x$
\end_inset

 out of conditional:
\begin_inset Formula 
\[
\left\{ p(y\mid\theta)\mid\theta\in\Theta\right\} 
\]

\end_inset


\end_layout

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Posterior Distribution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
prior distribution
\series default
 
\begin_inset Formula $p(\theta)$
\end_inset

 represents our beliefs about 
\begin_inset Formula $\theta$
\end_inset

 before seeing 
\begin_inset Formula $\cd$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
posterior distribution 
\series default
for 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
p(\theta\mid\cd,x)\pause & \propto & p(\cd\mid\theta,x)p(\theta)\\
\pause & = & \underbrace{L_{\cd}(\theta)}_{\text{likelihood}}\underbrace{p(\theta)}_{\text{prior }}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Posterior represents the
\series bold
 rationally 
\begin_inset Quotes eld
\end_inset

updated
\begin_inset Quotes erd
\end_inset

 beliefs
\series default
 after seeing 
\begin_inset Formula $\cd$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Each 
\begin_inset Formula $\theta$
\end_inset

 corresponds to a prediction function,
\end_layout

\begin_deeper
\begin_layout Itemize
i.e.
 the conditional distribution function 
\begin_inset Formula $p(y\mid x,\theta)$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Pause

\end_layout

\begin_layout Itemize
With MLE, we choose a single 
\begin_inset Formula $\theta$
\end_inset

 and our prediction function is
\begin_inset Formula 
\[
p\left(y\mid x,\hat{\theta}_{\text{MLE}}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
What can we do in the Bayesian setting?
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Point Estimates of Parameter
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose for some reason we want point estimates of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can use Bayesian decision theory to derive point estimates.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
As discussed last week, we may want to use
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\theta}=\ex\left[\theta\mid\cd,x\right]$
\end_inset

 (the posterior mean estimate)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\theta}=\text{median}[\theta\mid\cd,x]$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{\theta}=\argmax_{\theta\in\Theta}p(\theta\mid\cd,x)$
\end_inset

 (the MAP estimate)
\end_layout

\end_deeper
\begin_layout Itemize
depending on our loss function.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Back to the basic question
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Find a function takes input 
\begin_inset Formula $x\in\cx$
\end_inset

 and produces a 
\series bold
distribution
\series default
 on 
\begin_inset Formula $\cy$
\end_inset

? 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Recall frequentist approach: 
\end_layout

\begin_deeper
\begin_layout Itemize
Choose family of conditional probability densities (hypothesis space).
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Select one conditional probability from family, e.g.
 by MLE.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
(MLE has nice properties, so a common choice.
 See advanced statistics class.)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian Prediction Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In Bayesian setting, 
\series bold
there is no selection
\series default
 from hypothesis space.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We chose a parametric family of conditional densities
\begin_inset Formula 
\[
\left\{ p(y\mid x,\theta):\theta\in\Theta\right\} ,
\]

\end_inset


\end_layout

\begin_layout Itemize
and a prior distribution 
\begin_inset Formula $p(\theta)$
\end_inset

 on this set.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose we get an 
\begin_inset Formula $x$
\end_inset

 and we need to predict a distribution for the corresponding 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Having set our Bayesian model, there are no more decisions to make – just
 computation...
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
There's no question of whether the 
\begin_inset Formula $p(y\mid x)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In Bayesian setting, the conditional distribution of 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

, just compute it conditional on everything we've observed...
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Prior Predictive Distribution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have not yet observed any data.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
In Bayesian setting, we can still produce a prediction function.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
prior predictive distribution 
\series default
is given by
\begin_inset Formula 
\[
x\mapsto p(y\mid x)\pause=\int p(y\mid x;\theta)p(\theta)\,d\theta.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is an average of all conditional densities in our family, weighted
 by the prior.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Such an average is also called a 
\series bold
mixture distribution
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Posterior Predictive Distribution 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we've already seen data 
\begin_inset Formula $\cd$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
posterior predictive distribution 
\series default
is given by
\begin_inset Formula 
\[
x\mapsto p(y\mid x,\cd)\pause=\int p(y\mid x;\theta)p(\theta\mid\cd)\,d\theta.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is an average of all conditional densities in our family, weighted
 by the posterior.
 
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
This is not the only predictive distribution function we can use after seeing
 
\begin_inset Formula $\cd$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The MAP estimator is another one we'll discuss later.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparison to Frequentist Approach
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In Bayesian statistics we have two distributions on 
\begin_inset Formula $\Theta$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
the prior distribution 
\begin_inset Formula $p(\theta)$
\end_inset

 
\end_layout

\begin_layout Itemize
the posterior distribution 
\begin_inset Formula $p(\theta\mid\cd$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We also think of these as distributions on the hypothesis space
\begin_inset Formula 
\[
\left\{ p(y\mid x,\theta):\theta\in\Theta\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In frequentist approach, we choose 
\begin_inset Formula $\hat{\theta}\in\Theta$
\end_inset

, and predict 
\begin_inset Formula 
\[
p(y\mid x,\hat{\theta}(\cd)).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In Bayesian approach, we integrate out over 
\begin_inset Formula $\Theta$
\end_inset

 w.r.t.
 
\begin_inset Formula $p(\theta\mid\cd$
\end_inset

) and predict with 
\begin_inset Formula 
\[
p(y\mid x,\cd)=\int p(y\mid x;\theta)p(\theta\mid\cd)\,d\theta
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What if we don't want a full distribution on 
\begin_inset Formula $y$
\end_inset

?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Once we have a predictive distribution 
\begin_inset Formula $p(y\mid x,\cd)$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
we can easily generate single point predictions.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
\begin_inset Formula $x\mapsto\ex\left[y\mid x,\cd\right]$
\end_inset

, to minimize expected square error.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $x\mapsto\text{median}[y\mid x,\cd]$
\end_inset

, to minimize expected absolute error
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $x\mapsto\argmax_{y\in\cy}p(y\mid x,\cd)$
\end_inset

, to minimize expected 
\begin_inset Formula $0/1$
\end_inset

 loss
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Each of these can be derived from 
\begin_inset Formula $p(y\mid x,\cd)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Gaussian Regression Example
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example in 1-Dimension: Setup
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=[-1,1]$
\end_inset


\begin_inset space \qquad{}
\end_inset

Output space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $x$
\end_inset

, the world generates 
\begin_inset Formula $y$
\end_inset

 as 
\begin_inset Formula 
\begin{eqnarray*}
y & = & w_{0}+w_{1}x+\eps,
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\eps\sim\cn(0,0.2^{2})$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Written another way, the 
\series bold
conditional probability model
\series default
 is
\begin_inset Formula 
\begin{eqnarray*}
y\mid x,w_{0},w_{1} & \sim & \cn\left(w_{0}+w_{1}x\,,\,0.2^{2}\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
What's the parameter space? 
\begin_inset Formula $\pause\reals^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Prior distribution:
\series default
 
\begin_inset Formula $w=\left(w_{0},w_{1}\right)\sim\cn\left(0,\frac{1}{2}I\right)$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example in 1-Dimension: Prior Situation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Prior distribution:
\series default
 
\begin_inset Formula $w=\left(w_{0},w_{1}\right)\sim\cn\left(0,\frac{1}{2}I\right)$
\end_inset

 (Illustrated on left)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/bayesian-methods/lin-regression-prior-PRMLFig3-7.png
	lyxscale 30
	width 60text%

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
On right,  
\begin_inset Formula $y(x)=\ex\left[y\mid x,w\right]=w_{0}+w_{1}x$
\end_inset

, for randomly chosen 
\begin_inset Formula $w\sim p(w)=\cn\left(0,\frac{1}{2}I\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Bishop's PRML Fig 3.7}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example in 1-Dimension: 1 Observation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/bayesian-methods/lin-regression-prior-PRMLFig3-7.1pt.png
	lyxscale 30
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
On left: posterior distribution; white '+' indicates true parameters
\end_layout

\begin_layout Itemize
On right: blue circle indicates the training observation
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Bishop's PRML Fig 3.7}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example in 1-Dimension: 2 and 20 Observations
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/bayesian-methods/lin-regression-prior-PRMLFig3-7.2and20pt.png
	lyxscale 30
	height 70theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Bishop's PRML Fig 3.7}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Gaussian Regression Continued
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closed Form for Posterior
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Model:
\begin_inset Formula 
\begin{eqnarray*}
w & \sim & \cn\left(0,\Sigma_{0}\right)\\
\pause y_{i}\mid x,w & \text{i.i.d.} & \cn(w^{T}x_{i},\sigma^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Design matrix 
\begin_inset Formula $X$
\end_inset

 
\begin_inset Formula $\qquad$
\end_inset

Response column vector 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Posterior distribution is a Gaussian distribution:
\begin_inset Formula 
\begin{eqnarray*}
w\mid\cd & \sim & \pause\cn(\mu_{P},\Sigma_{P})\\
\mu_{\text{P}} & = & \left(X^{T}X+\sigma^{2}\Sigma_{0}^{-1}\right)^{-1}X^{T}y\\
\Sigma_{\text{P}} & = & \left(\sigma^{-2}X^{T}X+\Sigma_{0}^{-1}\right)^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Posterior Variance
\series default
 
\begin_inset Formula $\Sigma_{P}$
\end_inset

 gives us a natural 
\series bold
uncertainty measure.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{See Rasmussen and Williams' 
\backslash
emph{Gaussian Processes for Machine Learning}, Ch 2.1.
 
\backslash
url{http://www.gaussianprocess.org/gpml/chapters/RW2.pdf}}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closed Form for Posterior
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Posterior distribution is a Gaussian distribution:
\begin_inset Formula 
\begin{eqnarray*}
w\mid\cd & \sim & \pause\cn(\mu_{P},\Sigma_{P})\\
\mu_{\text{P}} & = & \left(X^{T}X+\sigma^{2}\Sigma_{0}^{-1}\right)^{-1}X^{T}y\\
\Sigma_{\text{P}} & = & \left(\sigma^{-2}X^{T}X+\Sigma_{0}^{-1}\right)^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If we want point estimates of 
\begin_inset Formula $w$
\end_inset


\series bold
, MAP estimator
\series default
 and the 
\series bold
posterior mean
\series default
 are given by
\begin_inset Formula 
\[
\pause\hat{w}=\mu_{P}=\left(X^{T}X+\sigma^{2}\Sigma_{0}^{-1}\right)^{-1}X^{T}y
\]

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For the prior variance 
\begin_inset Formula $\Sigma_{0}=\frac{\sigma^{2}}{\lambda}I$
\end_inset

, we get
\begin_inset Formula 
\[
\hat{w}=\mu_{P}=\left(X^{T}X+\lambda I\right)^{-1}X^{T}y,\pause
\]

\end_inset

which is of course the ridge regression solution.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior Variance vs.
 Traditional Uncertainty
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Traditional regression: OLS estimator (also the MLE) is a random variable
 – why? 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Because estimator is a function of data 
\begin_inset Formula $\cd$
\end_inset

 and data is random.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Common assumption: data are iid with Gaussian noise: 
\begin_inset Formula $y=w^{T}x+\eps$
\end_inset

, with 
\begin_inset Formula $\eps\sim\cn\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Then OLS estimator 
\begin_inset Formula $\hat{w}$
\end_inset

 has a 
\series bold
sampling distribution
\series default
 that is Gaussian with mean 
\begin_inset Formula $w$
\end_inset

 and
\begin_inset Formula 
\[
\cov(\hat{w})=\left(\sigma^{-2}X^{T}X\right)^{-1}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
By comparison, the posterior variance is
\begin_inset Note Note
status open

\begin_layout Plain Layout
should also note that the posterior mean is different than the expected
 value of the MLE 
\begin_inset Formula $\hat{w}$
\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\[
\Sigma_{P}=\left(\sigma^{-2}X^{T}X+\Sigma_{0}^{-1}\right)^{-1}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When we take 
\begin_inset Formula $\Sigma_{0}^{-1}=0$
\end_inset

, we get back 
\begin_inset Formula $\cov(\hat{\theta})$
\end_inset

 (i.e.
 like our prior variance goes to 
\begin_inset Formula $\infty$
\end_inset

.
 )
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Sigma_{P}$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

smaller
\begin_inset Quotes erd
\end_inset

 than 
\begin_inset Formula $\cov(\hat{w})$
\end_inset

 because we're using a 
\begin_inset Quotes eld
\end_inset

more informative
\begin_inset Quotes erd
\end_inset

 prior.
 
\end_layout

\end_deeper
\end_inset


\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior Mean and Posterior Mode (MAP)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let's find 
\begin_inset Formula $\hat{w}_{\text{MAP}}$
\end_inset

 another way to elaborate on connection to ridge.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Posterior density 
\series default
on 
\begin_inset Formula $w$
\end_inset


\series bold
 
\series default
for
\series bold
 
\series default

\begin_inset Formula $\Sigma_{0}=\frac{\sigma^{2}}{\lambda}I$
\end_inset

:
\series bold

\begin_inset Formula 
\begin{eqnarray*}
p(w\mid\cd) & \propto & \underbrace{\exp\left(-\frac{\lambda}{2\sigma^{2}}\|w\|^{2}\right)}_{\text{prior}}\underbrace{\prod_{i=1}^{n}\exp\left(-\frac{(y_{i}-w^{T}x_{i})^{2}}{2\sigma^{2}}\right)}_{\text{likelihood}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To find 
\series bold
MAP
\series default
, sufficient to minimize the negative log posterior:
\begin_inset Formula 
\begin{eqnarray*}
\hat{w}_{\text{MAP}} & = & \argmin_{w\in\reals^{d}}\left[-\log p(w\mid\cd)\right]\\
\pause & = & \argmin_{w\in\reals^{d}}\underbrace{\sum_{i=1}^{n}(y_{i}-w^{T}x_{i})^{2}}_{\text{log-likelihood}}+\underbrace{\lambda\|w\|^{2}}_{\text{log-prior}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Which is the ridge regression objective.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Predictive Distribution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Given a new input point 
\begin_inset Formula $x_{\text{new}}$
\end_inset

, how to predict 
\begin_inset Formula $y_{\text{new}}$
\end_inset

 ?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Predictive distribution
\begin_inset Formula 
\begin{eqnarray*}
p(y_{\text{new}}\mid x_{\text{new}},\cd) & = & \pause\int p(y_{\text{new}}\mid x_{\text{new}},w,\cd)p(w\mid\cd)\,dw\\
\pause & = & \int p(y_{\text{new}}\mid x_{\text{new}},w)p(w\mid\cd)\,dw
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For Gaussian regression, predictive distribution has closed form.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closed Form for Predictive Distribution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Model
\series default
:
\begin_inset Formula 
\begin{eqnarray*}
w & \sim & \cn\left(0,\Sigma_{0}\right)\\
\pause y_{i}\mid x,w & \text{i.i.d.} & \cn(w^{T}x_{i},\sigma^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Predictive Distribution
\series default

\begin_inset Formula 
\begin{eqnarray*}
p(y_{\text{new}}\mid x_{\text{new}},\cd) & = & \int p(y_{\text{new}\text{ }}\mid x_{\text{new}},w)p(w\mid\cd)\,dw.
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Averages over prediction for each 
\begin_inset Formula $w$
\end_inset

, weighted by posterior distribution.
\end_layout

\begin_deeper
\begin_layout Pause
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Closed form:
\series default

\begin_inset Formula 
\begin{eqnarray*}
y_{\text{new}}\mid x_{\text{new}},\cd & \sim & \cn\left(\eta_{\text{new}}\,,\,\sigma_{\text{new}}^{2}\right)\\
\pause\eta_{\text{new}} & = & \mu_{\text{P}}^{T}x_{\text{new}}\\
\pause\sigma_{\text{new}}^{2} & = & \underbrace{x_{\text{new}}^{T}\Sigma_{\text{P}}x_{\text{new}}}_{\text{from variance in }w}+\underbrace{\sigma^{2}}_{\text{inherent variance in }y}
\end{eqnarray*}

\end_inset


\begin_inset Note Note
status open

\begin_layout Pause
 
\end_layout

\begin_layout Itemize
(In frequentist framework, could get same thing by bootstrap.)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Predictive Distributions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
With predictive distributions, can give mean prediction with error bands:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/bayesian-methods/predictiveDistWithErrorBands.png
	lyxscale 60
	height 70theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Rasmussen and Williams' 
\backslash
emph{Gaussian Processes for Machine Learning}, Fig.2.1(b) }}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_body
\end_document
