#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Kernel Methods
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 / CSCI-GA 2567
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
Julia Kempe & David S.
 Rosenberg 
\end_layout

\begin_layout Date
February 19, 2019
\end_layout

\begin_layout Institute
CDS, NYU
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Contents
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
two main points of kernel stuff:
\end_layout

\begin_layout Plain Layout
1) kernelization, 
\end_layout

\begin_layout Plain Layout
TODO:
\end_layout

\begin_layout Plain Layout
URGENT: using 3 notations to specify to elements of input space: 
\begin_inset Formula $x,w$
\end_inset

, 
\begin_inset Formula $x^{(1)},x^{(2)}$
\end_inset

 and 
\begin_inset Formula $x,x'$
\end_inset

.
 Choose one and stick to it.
 Do NOT choose 
\begin_inset Formula $x,w$
\end_inset

 , because 
\begin_inset Formula $w$
\end_inset

 is our weight vector, which is very confusing.
 
\end_layout

\begin_layout Plain Layout
1) definition of 
\begin_inset Quotes eld
\end_inset

kernelized
\begin_inset Quotes erd
\end_inset

 – should we extend it? what about prediction? 
\end_layout

\begin_layout Plain Layout
2) work on transitions between SVM, linear kernel, kernel matrix – motivations
 etc.
\end_layout

\begin_layout Plain Layout
Maybe define the kernel matix in terms of kernel evaluations first and then
 plug in the linear inner product? Or introduce the Gram matrix of linear
 inner products very early on, and then later say we can replace it with
 a kernel matrix of inner products for a nonlinear inner product?
\end_layout

\begin_layout Plain Layout
3) I did some board work to show that the gram matri is XX^T.
 (XX^T)_{ij} = ? discussion of dimensionality nxn vs dxd to decide between
 kernel methods and primal form
\end_layout

\begin_layout Plain Layout
4) would be cool to demonstrate some kernel matrix substitutions and the
 effects on the output – maybe pull from the homework?
\end_layout

\begin_layout Plain Layout
5) the quadratic kernel shows up twice with different notations?
\end_layout

\begin_layout Plain Layout
6) discussion of coefficients of the monomials in the equivalent feature
 maps fro the polynomial kernels; these coefficients interact with the regulariz
ation;
\end_layout

\begin_layout Plain Layout
7) board work to explain what it would mean to have RBF kernel representable
 as an 
\begin_inset Quotes eld
\end_inset

inner product of feature vectors
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
8) very interesting treatment of the interaction between the kernel and
 the regularization described in Scholkopf and Smola's book 
\begin_inset Quotes eld
\end_inset

Learning with kernels
\begin_inset Quotes erd
\end_inset

 SECTION 4.3.
 
\end_layout

\begin_layout Plain Layout
9) need to rehearse the kernel trick summary
\end_layout

\begin_layout Plain Layout
10) are there situations where we can kernelize, but the representer theorem
 doesn't work? 
\end_layout

\begin_layout Plain Layout
11) should we break out the deck on representer theorem? YES [done]
\end_layout

\begin_layout Plain Layout
12) prove that l1 norm does not obey the parallelogram law [Homework?]
\end_layout

\begin_layout Plain Layout
13) illustrated projection on the board of 
\begin_inset Formula $x$
\end_inset

 onto subspace 
\begin_inset Formula $M$
\end_inset

.
 [Would be nice to have an image of that in the slides, just for the record]
\end_layout

\begin_layout Plain Layout
14) In looking at the 
\begin_inset Quotes eld
\end_inset

generalized objective
\begin_inset Quotes erd
\end_inset

 form, we say 
\begin_inset Quotes eld
\end_inset

what is 
\begin_inset Formula $\left\langle w,\psi(x_{i})\right\rangle $
\end_inset

 – it should be familiar.
 It's the score on 
\begin_inset Formula $x_{i}$
\end_inset

.
 So we have a general function L of the scores/predictions.
 Where are the labels 
\begin_inset Formula $y_{i}$
\end_inset

? theyr'e built into the function 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Plain Layout
15) I need some rhyme or reason to when I use raw elements of input space
 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $x_{j}$
\end_inset

 and when I use 
\begin_inset Formula $\phi(x_{i})$
\end_inset

 and 
\begin_inset Formula $\phi(x_{j})$
\end_inset

.
 I think I can take a few slides to explain that generically 
\begin_inset Formula $\cx$
\end_inset

 is arbitrary, but all our methods use 
\begin_inset Formula $\reals^{d}$
\end_inset

.
 So for simplicity we'll usually just say 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 .
 But sometimes, e.g.
 in kernel context, it makes sense to talk abou kernels acting directly
 on 
\begin_inset Formula $\cx$
\end_inset

.
 So really only in kernel context is it worth taling about 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Big Feature Spaces for Linear Models
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Input Space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Our general learning theory setup: no assumptions about 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 for the specific methods we've developed: 
\end_layout

\begin_deeper
\begin_layout Itemize
Ridge regression
\end_layout

\begin_layout Itemize
Lasso regression
\end_layout

\begin_layout Itemize
Support Vector Machines 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Our hypothesis space for these was all affine functions on 
\begin_inset Formula $\reals^{d}$
\end_inset

:
\begin_inset Formula 
\[
\cf=\left\{ x\mapsto w^{T}x+b\mid w\in\reals^{d},b\in\reals\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we want to do prediction on inputs not natively in 
\begin_inset Formula $\reals^{d}$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Input Space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Often want to use inputs not natively in 
\begin_inset Formula $\reals^{d}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Text documents
\end_layout

\begin_layout Itemize
Image files
\end_layout

\begin_layout Itemize
Sound recordings
\end_layout

\begin_layout Itemize
DNA sequences
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
But everything in a computer is a sequence of numbers?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $i$
\end_inset

th entry of each sequence should have the same 
\begin_inset Quotes eld
\end_inset

meaning
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
All the sequences should have the same length
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Extraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
Mapping an input from 
\begin_inset Formula $\cx$
\end_inset

 to a vector in 
\begin_inset Formula $\reals^{d}$
\end_inset

 is called 
\series bold
feature extraction
\series default
 or 
\series bold
featurization
\series default
.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Archive/2015/Lectures/source/4a.kernels-highlevel/feature-extraction.png
	lyxscale 60
	width 90text%

\end_inset


\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 Quadratic feature map: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}.
\]

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Models with Explicit Feature Map
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx$
\end_inset

 (no assumptions)
\end_layout

\begin_layout Itemize
Introduce 
\series bold
feature map
\series default
 
\begin_inset Formula $\psi:\cx\to\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
The feature map maps into the 
\series bold
feature space
\series default
 
\begin_inset Formula $\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis space of affine functions on feature space:
\begin_inset Formula 
\[
\cf=\left\{ x\mapsto w^{T}\psi(x)+b\mid w\in\reals^{d},b\in\reals\right\} .
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Geometric Example: Two class problem, nonlinear boundary
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/features/circularBoundary.png
	lyxscale 30
	height 50theight%

\end_inset


\end_layout

\begin_layout Itemize
With identity feature map 
\begin_inset Formula $\psi(x)=\left(x_{1},x_{2}\right)$
\end_inset

 and linear models, can't separate regions
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
With appropriate featurization 
\begin_inset Formula $\psi(x)=\left(x_{1},x_{2},x_{1}^{2}+x_{2}^{2}\right)$
\end_inset

, becomes linearly separable .
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Video: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://youtu.be/3liCbRZPrZA
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Expressivity of Hypothesis Space
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For linear models, to grow the hypothesis spaces, we must add features.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Sometimes we say a larger hypothesis is 
\begin_inset Quotes eld
\end_inset

more expressive
\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
(can fit more relationships between input and action)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Many ways to create new features.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Handling with Nonlinearity with Linear Methods
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example Task: Predicting Health
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
General Philosophy: Extract every feature that might be relevant
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Features for medical diagnosis
\end_layout

\begin_deeper
\begin_layout Itemize
height
\end_layout

\begin_layout Itemize
weight
\end_layout

\begin_layout Itemize
body temperature
\end_layout

\begin_layout Itemize
blood pressure
\end_layout

\begin_layout Itemize
etc...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Issues for Linear Predictors
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For linear predictors, it's important 
\series bold
how
\series default
 features are added
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Three types of nonlinearities can cause problems:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Enumerate
Non-monotonicity
\end_layout

\begin_layout Enumerate
Saturation
\end_layout

\begin_layout Enumerate
Interactions between features
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-monotonicity: The Issue
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Feature Map: 
\begin_inset Formula $\phi(x)=\left[1,\text{temperature}(x)\right]$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Action: Predict health score 
\begin_inset Formula $y\in\reals$
\end_inset

 (positive is good)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis Space 
\begin_inset Formula $\cf{=}\left\{ \mbox{affine functions of temperature}\right\} $
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Issue: 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Health is not an affine function of temperature.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Affine function can either say
\end_layout

\begin_deeper
\begin_layout Itemize
Very high is bad and very low is good, or
\end_layout

\begin_layout Itemize
Very low is bad and very high is good,
\end_layout

\begin_layout Itemize
But here, both extremes are bad.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-monotonicity: Solution 1
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Transform the input:
\begin_inset Formula 
\[
\phi(x)=\left[1,\left\{ \text{temperature(x)-37}\right\} ^{2}\right],
\]

\end_inset

where 
\begin_inset Formula $37$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

normal
\begin_inset Quotes erd
\end_inset

 temperature in Celsius.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Ok, but requires manually-specified domain knowledge
\end_layout

\begin_deeper
\begin_layout Itemize
Do we really need that?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-monotonicity: Solution 2
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Think less, put in more:
\begin_inset Formula 
\[
\phi(x)=\left[1,\text{temperature}(x),\left\{ \text{temperature}(x)\right\} ^{2}\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
More expressive
\series default
 than Solution 1.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
General Rule
\end_layout

\end_inset


\end_layout

\begin_layout Block
Features should be simple building blocks that can be pieced together.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Saturation: The Issue
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Setting: Find products relevant to user's query
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Input: Product 
\begin_inset Formula $x$
\end_inset


\end_layout

\begin_layout Itemize
Action: Score the relevance of 
\begin_inset Formula $x$
\end_inset

 to user's query
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Feature Map:
\begin_inset Formula 
\[
\phi(x)=\left[1,N(x)\right],
\]

\end_inset

where 
\begin_inset Formula $N(x)=\text{number of people who bought }x$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We expect a monotonic relationship between 
\begin_inset Formula $N(x)$
\end_inset

 and relevance, but...
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Saturation: The Issue
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout AlertBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Is relevance linear in N(x)?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Relevance score reflects confidence in relevance prediction.
\end_layout

\begin_layout Itemize
Are we 10 times more confident if 
\begin_inset Formula $N(x)=1000$
\end_inset

 vs 
\begin_inset Formula $N(x)=100$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Bigger is better...
 but not that much better.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Saturation: Solve with nonlinear transform
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Smooth nonlinear transformation:
\begin_inset Formula 
\[
\phi(x)=\left[1,\log\left\{ 1+N(x)\right\} \right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\log\left(\cdot\right)$
\end_inset

 good for values with large dynamic ranges
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\emph on
Does it matter what base we use in the log?
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Saturation: Solve by discretization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Discretization (a discontinuous transformation):
\begin_inset Formula 
\[
\phi(x)=\left(\ind{0\le N(x)<10},\ind{10\le N(x)<100},\ldots\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Note Note
status open

\begin_layout Itemize
Sometimes we might prefer one-sided buckets (e.g.
 if the extreme buckets won't have much data):
\begin_inset Formula 
\[
\phi(x)=\left(\ind{N(x)\ge0},\ind{N(x)\ge10},\ind{N(x)\ge100}\ldots\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_inset

Small buckets allow quite flexible relationship
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Interactions: The Issue
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input: Patient information 
\begin_inset Formula $x$
\end_inset


\end_layout

\begin_layout Itemize
Action: Health score 
\begin_inset Formula $y\in\reals$
\end_inset

 (higher is better)
\end_layout

\begin_layout Itemize
Feature Map
\begin_inset Formula 
\[
\phi(x)=\left[\mbox{height}(x),\mbox{weight}(x)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Issue: It's the weight 
\series bold
relative
\series default
 to the height that's important.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Impossible to get with these features and a linear classifier.
\end_layout

\begin_layout Itemize
Need some 
\series bold
interaction
\series default
 between height and weight.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Interactions: Approach 1
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Google 
\begin_inset Quotes eld
\end_inset

ideal weight from height
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
J.
 D.
 Robinson's 
\begin_inset Quotes eld
\end_inset

ideal weight
\begin_inset Quotes erd
\end_inset

 formula (for a male):
\begin_inset Formula 
\[
\mbox{weight}\mbox{(kg)}=52+1.9\left[\mbox{height(in)}-60\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Make score square deviation between height(
\begin_inset Formula $h$
\end_inset

) and ideal weight(
\begin_inset Formula $w$
\end_inset

)
\begin_inset Formula 
\[
f(x)=\left(52+1.9\left[h(x)-60\right]-w(x)\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
WolframAlpha for complicated Mathematics:
\begin_inset Formula 
\[
f(x)=3.61h(x)^{2}-3.8h(x)w(x)-235.6h(x)+w(x)^{2}+124w(x)+3844
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Interactions: Approach 2
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Just include all second order features:
\begin_inset Formula 
\[
\phi(x)=\left[1,h(x),w(x),h(x)^{2},w(x)^{2},\underbrace{h(x)w(x)}_{\mbox{cross term}}\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
More flexible, no Google, no WolframAlpha.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
General Principle
\end_layout

\end_inset


\end_layout

\begin_layout Block
Simpler building blocks replace a single 
\begin_inset Quotes eld
\end_inset

smart
\begin_inset Quotes erd
\end_inset

 feature.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Predicate Features and Interaction Terms
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
predicate
\series default
 on the input space 
\begin_inset Formula $\cx$
\end_inset

 is a function 
\begin_inset Formula $P:\cx\to\left\{ \mbox{True},\mbox{False}\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Many features take this form:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\mapsto s(x)=\ind{\mbox{subject is sleeping}}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $x\mapsto d(x)=\ind{\mbox{subject is driving}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For predicates, interaction terms correspond to 
\series bold
AND
\series default
 conjunctions:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\mapsto s(x)d(x)=\ind{\mbox{subject is sleeping AND subject is driving}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example: Monomial Interaction Terms
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we start with 
\begin_inset Formula $x=\left(1,x_{1},\ldots,x_{d}\right)\in\reals^{d+1}=\cx$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
To get a more expressive hypothesis space, we want to add interaction terms.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Consider adding all monomials of degree 
\begin_inset Formula $M$
\end_inset

: 
\begin_inset Formula $x_{1}^{p_{1}}\cdots x_{d}^{p_{d}}$
\end_inset

, with 
\begin_inset Formula $p_{1}+\cdots+p_{d}=M$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
How many features will we end up with?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula ${M+d-1 \choose M}$
\end_inset

 (
\begin_inset Quotes eld
\end_inset

flower shop problem
\begin_inset Quotes erd
\end_inset

 from combinatorics)
\begin_inset Note Note
status open

\begin_layout Plain Layout
(M+d-1) c (d-1) = (M+d-1) c (M).
 If we wanted to have 
\begin_inset Formula $p_{1}+\cdots+p_{d}\le M$
\end_inset

, then that's equivalent to having an extra variable 
\begin_inset Formula $p_{d+1}=1$
\end_inset

, so it doesn't increase the degree.
 In which case the number of these monomials is 
\begin_inset Formula $\binom{M+d}{M}=\binom{M+d}{d}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $d=40$
\end_inset

 and 
\begin_inset Formula $M=8$
\end_inset

, we get 
\begin_inset Formula $314457495$
\end_inset

 features.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
That will make some extremely large data matrices...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Big Feature Spaces
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Very large feature spaces have two potential issues:
\end_layout

\begin_layout Enumerate
Overfitting
\end_layout

\begin_layout Enumerate
Memory and computational costs 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Overfitting we handle with regularization.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset


\series bold
Kernel methods
\series default

\begin_inset Quotes erd
\end_inset

 can (sometimes) help with memory and computational costs.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
In practice, kernel methods most applicable for linear methods with 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Kernel Methods: Motivation
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM with Explicit Feature Map
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\psi:\cx\to\reals^{d}$
\end_inset

 be a feature map.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The SVM optimization problem (with explicit feature map):
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}w^{T}\psi(x_{i})\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Last time we mentioned an equivalent optimization problem from Lagrangian
 duality...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM Dual Problem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
By Lagrangian duality, it is equivalent to solve the following optimization
 problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{\alpha\in\reals^{n}} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}\psi\left(x_{j}\right)^{T}\psi(x_{i})\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\qquad\text{and}\qquad\alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\alpha^{*}$
\end_inset

 is an optimal value, then
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\psi(x_{i})\qquad\text{and}\qquad\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\psi(x_{i})^{T}\psi(x).
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Notice: 
\begin_inset Formula $\psi\left(x\right)$
\end_inset

 only shows up in an inner products with another 
\begin_inset Formula $\psi(x')$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Methods Can Be 
\begin_inset Quotes eld
\end_inset

Kernelized
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if every feature vector 
\begin_inset Formula $\psi(x)$
\end_inset

 only appears inside an inner product with another feature vector 
\begin_inset Formula $\psi(x')$
\end_inset

.
 This applies to both the optimization problem and the prediction function.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The SVM Dual is a kernelization of the original SVM formulation.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We'll now introduce some special notation for these inner products 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Input space
\series default
: 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature space
\series default
: 
\begin_inset Formula $\ch$
\end_inset

 (a Hilbert space, i.e.
 an inner product space with projections, e.g.
 
\begin_inset Formula $\reals^{d}$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
a pre-Hilbert space (i.e.
 not necessarily complete) can still have projections, in the sense of a
 vector that's closer than all the others with residual orthogonal to subspace...
 but need Hilbert space to guarantee existence
\end_layout

\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature map
\series default
: 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 is 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle ,
\]

\end_inset

where 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is the inner product associated with 
\begin_inset Formula $\ch$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Function: Why do we need this?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Feature map
\series default
: 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset


\end_layout

\begin_layout Itemize
The 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 is 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle .
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Why introduce this new notation 
\begin_inset Formula $k(x,x')$
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We can often evaluate 
\begin_inset Formula $k(x,x')$
\end_inset

 without explicitly computing 
\begin_inset Formula $\psi(x)$
\end_inset

 and 
\begin_inset Formula $\psi(x')$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
For large feature spaces, can be much faster.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Evaluation Can Be Fast
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Example
Quadratic feature map for 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{d}\right)\in\reals^{d}$
\end_inset

.
\begin_inset Formula 
\[
\ensuremath{\psi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset

has dimension 
\begin_inset Formula $O(d^{2})\pause$
\end_inset

, but for any 
\begin_inset Formula $x,x'\in\reals^{d}$
\end_inset

 and the standard Euclidean dot products, 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle =\left\langle x,x'\right\rangle +\left\langle x,x'\right\rangle ^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Explicit computation of 
\begin_inset Formula $k(x,x')$
\end_inset

: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Implicit computation of 
\begin_inset Formula $k(x,x')$
\end_inset

: 
\begin_inset Formula $O(d)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernels as Similarity Scores
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Often useful to think of the kernel function as a 
\series bold
similarity score
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
But this is not a mathematically precise statement.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
There are many ways to design a similarity score.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We will use kernel functions that correspond to inner products in some feature
 space.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
These are called 
\series bold
Mercer kernels.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What are the Benefits of Kernelization?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Computational (when optimizing over 
\begin_inset Formula $\reals^{n}$
\end_inset

 is better than over 
\begin_inset Formula $\reals^{d}$
\end_inset

)
\begin_inset Note Note
status open

\begin_layout Plain Layout
(WAIT– not sure we're prepared yet for this n vs d claim)
\end_layout

\end_inset

).
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can sometimes avoid any 
\begin_inset Formula $O(d)$
\end_inset

 operations
\end_layout

\begin_deeper
\begin_layout Itemize
allows access to
\series bold
 infinite-dimensional feature spaces
\series default
.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Allows thinking in terms of 
\begin_inset Quotes eld
\end_inset

similarity
\begin_inset Quotes erd
\end_inset

 rather than features.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
(debatable)
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
kernel matrix
\series default
 for a kernel 
\begin_inset Formula $k$
\end_inset

 on 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\cx$
\end_inset

 is
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\in\reals^{n\times n}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In ML this is also called a 
\series bold
Gram matrix
\series default
, but traditionally (in linear algebra),
\end_layout

\begin_deeper
\begin_layout Itemize
Gram matrices are defined without reference to a kernel or feature map.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The kernel matrix summarizes all the information we need about the training
 inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 to solve a kernelized optimization problem.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 in the kernelized SVM, we can replace 
\begin_inset Formula $\psi(x_{i})^{T}\psi(x_{j})$
\end_inset

 with 
\begin_inset Formula $K_{ij}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}K_{ij}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\qquad\text{and}\qquad\alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The 
\begin_inset Quotes eld
\end_inset

Kernel Trick
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Given a kernelized ML algorithm (i.e.
 all 
\begin_inset Formula $\psi(x)$
\end_inset

's show up as 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can swap out the inner product for a new kernel function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
New kernel may correspond to a very high-dimensional feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Once the kernel matrix is computed, the computational cost depends on number
 of data points, rather than the dimension of feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
The 
\series bold
trick
\series default
 is that once you've implemented your method in terms of a kernel matrix,
 you can go from a kernel corresponding to a very small feature vector to
 a kernel corresponding to a very large (even infinite dimensional) feature
 vector, without changing your code, just by swapping one kernel matrix
 for another.
 Runtime is unaffected, after the kernel matrix is computed.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Our Plan
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Present our principal tool for kernelization: the 
\series bold
representer theorem
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
To keep things clean, we'll drop the explicit feature map until we need
 it: 
\begin_inset Formula $\psi(x)=x$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Discuss specific cases of kernel ridge regression and kernel SVM
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Discuss several kernels, including the famous RBF kernel.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Discuss how to create a kernel without an explicit feature map.
\end_layout

\end_deeper
\begin_layout Section
The Representer Theorem to Kernelize
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status collapsed

\begin_layout Plain Layout
The Representer Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Representer Theorem]
\end_layout

\end_inset

 Let 
\begin_inset Formula 
\[
J(w)=R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right),
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $w,x_{1},\ldots,x_{n}\in\ch$
\end_inset

 for some Hilbert space 
\begin_inset Formula $\ch$
\end_inset

.
 (We typically have 
\begin_inset Formula $\ch=\reals^{d}.)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\|\cdot\|$
\end_inset

 is the norm corresponding to the inner product of 
\begin_inset Formula $\ch$
\end_inset

.
 (i.e.
 
\begin_inset Formula $\|w\|=\sqrt{\left\langle w,w\right\rangle }$
\end_inset

) 
\end_layout

\begin_layout Itemize
\begin_inset Formula $R:[0,\infty)\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
), and
\end_layout

\begin_layout Itemize
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary (
\series bold
Loss term
\series default
).
\end_layout

\end_deeper
\begin_layout Theorem
If 
\begin_inset Formula $J(w)$
\end_inset

 has a minimizer, then it 
\series bold
has a minimizer of the form
\series default
 
\begin_inset Formula $w^{*}=\sum_{i=1}^{n}\alpha_{i}x_{i}.$
\end_inset


\begin_inset Newline newline
\end_inset

[If 
\begin_inset Formula $R$
\end_inset

 is strictly increasing, then all minimizers have this form.
 (Proof in homework.)]
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
[If 
\begin_inset Formula $w_{M}^{*}$
\end_inset

 is a minimizer of 
\begin_inset Formula $J(w)$
\end_inset

 constrained to 
\begin_inset Formula $M=\linspan\left(x_{1},\ldots,x_{n}\right)$
\end_inset

 then it is also an unconstrained minimizer of 
\begin_inset Formula $J(w)$
\end_inset

.
 Proof again by representer theorem type argument.
 But note that if 
\begin_inset Formula $\alpha^{*}$
\end_inset

 is a minimum of 
\begin_inset Formula $J$
\end_inset

 over 
\begin_inset Formula $\alpha$
\end_inset

's, then it must also be a minimum over all 
\begin_inset Formula $w$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Rewriting the Objective Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Define the training score function 
\begin_inset Formula $s:\reals^{d}\to\reals^{n}$
\end_inset

 by
\begin_inset Formula 
\[
s(w)=\begin{pmatrix}\left\langle w,x_{1}\right\rangle \\
\vdots\\
\left\langle w,x_{n}\right\rangle 
\end{pmatrix},
\]

\end_inset

which gives the 
\series bold
training score vector
\series default
 for any 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We can then rewrite the objective function as
\begin_inset Formula 
\[
J(w)=R\left(\|w\|\right)+L\left(s(w)\right),
\]

\end_inset

where now 
\begin_inset Formula $L:\reals^{n\times1}\to\reals$
\end_inset

 takes a column vector as input.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
This will allow us to have a slick reparametrized version...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Reparametrize the Generalized Objective
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
By the Representer Theorem, it's sufficient to minimize 
\begin_inset Formula $J(w)$
\end_inset

 for 
\begin_inset Formula $w$
\end_inset

 of the form 
\begin_inset Formula $\sum_{i=1}^{n}\alpha_{i}x_{i}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Plugging this form into 
\begin_inset Formula $J(w)$
\end_inset

, we see we can just minimize
\begin_inset Formula 
\[
J_{0}(\alpha)=R\left(\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert \right)+L\left(s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right)\right)
\]

\end_inset

 over 
\begin_inset Formula $\alpha=\left(\alpha_{1},\ldots,\alpha_{n}\right)^{T}\in\reals^{n\times1}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
With some new notation, we can substantially simplify 
\end_layout

\begin_deeper
\begin_layout Itemize
the norm piece 
\begin_inset Formula $\|w\|=\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert $
\end_inset

, and
\end_layout

\begin_layout Itemize
the score piece 
\begin_inset Formula $s(w)=s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right)$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simplifying the Reparametrized Norm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For the norm piece 
\begin_inset Formula $\|w\|=\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert $
\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray*}
\pause\|w\|^{2} & = & \left\langle w,w\right\rangle \\
 & = & \left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},\sum_{j=1}^{n}\alpha_{j}x_{j}\right\rangle \\
\pause & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}\left\langle x_{i},x_{j}\right\rangle .
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
This expression involves the 
\begin_inset Formula $n^{2}$
\end_inset

 inner products between all pairs of input vectors.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We often put those values together into a matrix...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gram Matrix
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
Gram matrix
\series default
 of a set of points 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 in an inner product space is defined as
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{i},x_{j}\right\rangle \end{pmatrix}_{i,j}=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is the traditional definition from linear algebra.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The Gram matrix is a special case of a 
\series bold
kernel matrix 
\series default
for the identity feature map.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
That's why we write 
\begin_inset Formula $K$
\end_inset

 for the Gram matrix instead of 
\begin_inset Formula $G$
\end_inset

, as done in elsewhere.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
NOTE: In ML, we often use Gram matrix and kernel matrix to mean the same
 thing.
 Don't get too hung up on the definitions.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example: Gram Matrix for the Dot Product
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\reals^{d\times1}$
\end_inset

 with the standard inner product 
\begin_inset Formula $\left\langle x,x'\right\rangle =x^{T}x'$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $X\in\reals^{n\times d}$
\end_inset

 be the 
\series bold
design matrix
\series default
, which has each input vector as a row: 
\begin_inset Formula 
\[
X=\begin{pmatrix}-x_{1}^{T}-\\
\vdots\\
-x_{n}^{T}-
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then the Gram matrix is
\begin_inset Formula 
\begin{eqnarray*}
K & = & \begin{pmatrix}x_{1}^{T}x_{1} & \cdots & x_{1}^{T}x_{n}\\
\vdots & \ddots & \cdots\\
x_{n}^{T}x_{1} & \cdots & x_{n}^{T}x_{n}
\end{pmatrix}\pause=\begin{pmatrix}-x_{1}^{T}-\\
\vdots\\
-x_{n}^{T}-
\end{pmatrix}\begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}\pause\\
 & = & XX^{T}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simplifying the Reparametrized Norm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
With 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$
\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray*}
\|w\|^{2} & = & \left\langle w,w\right\rangle \\
 & = & \left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},\sum_{j=1}^{n}\alpha_{j}x_{j}\right\rangle \\
 & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}\left\langle x_{i},x_{j}\right\rangle \pause\\
 & = & \alpha^{T}K\alpha.
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simplifying the Training Score Vector
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The score for 
\begin_inset Formula $x_{j}$
\end_inset

 for 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
\left\langle w,x_{j}\right\rangle  & = & \left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},x_{j}\right\rangle =\sum_{i=1}^{n}\alpha_{i}\left\langle x_{i},x_{j}\right\rangle 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The training score vector is
\begin_inset Formula 
\begin{eqnarray*}
\pause s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right) & = & \begin{pmatrix}\sum_{i=1}^{n}\alpha_{i}\left\langle x_{i},x_{1}\right\rangle \\
\vdots\\
\sum_{i=1}^{n}\alpha_{i}\left\langle x_{i},x_{n}\right\rangle 
\end{pmatrix}\pause=\begin{pmatrix}\alpha_{1}\left\langle x_{1},x_{1}\right\rangle +\cdots+\alpha_{n}\left\langle x_{n},x_{1}\right\rangle \\
\vdots\\
\alpha_{1}\left\langle x_{1},x_{n}\right\rangle +\cdots+\alpha_{n}\left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\\
\pause & = & \begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}\\
\pause & = & K\alpha
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Reparametrized Objective
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Putting it all together, our reparametrized objective function can be written
 as
\begin_inset Formula 
\begin{eqnarray*}
J_{0}(\alpha) & = & R\left(\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert \right)+L\left(s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right)\right)\\
 & = & R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right),
\end{eqnarray*}

\end_inset

which we minimize over 
\begin_inset Formula $\alpha\in\reals^{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
All information needed about 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 is summarized in the Gram matrix 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We're now minimizing over 
\begin_inset Formula $\reals^{n}$
\end_inset

 rather than 
\begin_inset Formula $\reals^{d}$
\end_inset

\SpecialChar endofsentence

\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $d\gg n$
\end_inset

, this can be a big win computationally (at least once 
\begin_inset Formula $K$
\end_inset

 is computed).
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
(Assumes 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

 do not hide any references to 
\begin_inset Formula $\psi(x_{i})$
\end_inset

.) 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Reparametrizing Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we've found 
\begin_inset Formula 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then we know 
\begin_inset Formula $w^{*}=\sum_{i=1}^{n}\alpha^{*}x_{i}$
\end_inset

 satisfies
\begin_inset Formula 
\[
w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The prediction on a new point 
\begin_inset Formula $x\in\ch$
\end_inset

 is
\begin_inset Formula 
\[
\hat{f}(x)=\left\langle w^{*},x\right\rangle \pause=\sum_{i=1}^{n}\alpha_{i}^{*}\left\langle x_{i},x\right\rangle .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To make a new prediction, we may need to touch all the training inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
More Notation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
It will be convenient to define the following column vector for any 
\begin_inset Formula $x\in\ch$
\end_inset

:
\begin_inset Formula 
\[
k_{x}=\begin{pmatrix}\left\langle x_{1},x\right\rangle \\
\vdots\\
\left\langle x_{n},x\right\rangle 
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then we can write our predictions on a new point 
\begin_inset Formula $x$
\end_inset

 as
\begin_inset Formula 
\[
\hat{f}(x)=k_{x}^{T}\alpha^{*}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Summary So Far
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Original plan: 
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)$
\end_inset


\end_layout

\begin_layout Itemize
Predict with 
\begin_inset Formula $\hat{f}(x)=\left\langle w^{*},x\right\rangle $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We showed that the following is equivalent:
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)$
\end_inset


\end_layout

\begin_layout Itemize
Predict with 
\begin_inset Formula $\hat{f}(x)=k_{x}^{T}\alpha^{*}\pause$
\end_inset

, where
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\qquad\text{and}\qquad k_{x}=\begin{pmatrix}\left\langle x_{1},x\right\rangle \\
\vdots\\
\left\langle x_{n},x\right\rangle 
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Every element 
\begin_inset Formula $x\in\ch$
\end_inset

 occurs inside an inner products with a training input 
\begin_inset Formula $x_{i}\in\ch$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Note that we could replace any 
\begin_inset Formula $x$
\end_inset

 with the projection of 
\begin_inset Formula $x$
\end_inset

 into 
\begin_inset Formula $\linspan\left(x_{1},\ldots,x_{n}\right)$
\end_inset

 without changing anything.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelization
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if every feature vector 
\begin_inset Formula $\psi(x)$
\end_inset

 only appears inside an inner product with another feature vector 
\begin_inset Formula $\psi(x')$
\end_inset

.
 This applies to both the optimization problem and the prediction function.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Here we are using 
\begin_inset Formula $\psi(x)=x$
\end_inset

.
 Thus finding 
\begin_inset Formula 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)
\]

\end_inset

 and making predictions with 
\begin_inset Formula $\hat{f}(x)=k_{x}^{T}\alpha^{*}$
\end_inset

 is a 
\series bold
kernelization
\series default
 of finding
\begin_inset Formula 
\[
w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]

\end_inset

 and making predictions with 
\begin_inset Formula $\hat{f}(x)=\left\langle w^{*},x\right\rangle $
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Now let's now consider some kernelizations of specific methods.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Once we have kernelized:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{f}(x)=k_{x}^{T}\alpha^{*}$
\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can do the 
\begin_inset Quotes eld
\end_inset

kernel trick
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Replace each 
\begin_inset Formula $\left\langle x,x'\right\rangle $
\end_inset

 by 
\begin_inset Formula $k(x,x')$
\end_inset

, for any kernel function 
\begin_inset Formula $k$
\end_inset

, where 
\begin_inset Formula $k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Predictions 
\begin_inset Formula 
\[
\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}k(x_{i},x)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Kernel Ridge Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelizing Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ridge Regression:
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\frac{1}{n}\|Xw-y\|^{2}+\lambda\|w\|^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Plugging in 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$
\end_inset

, we get the kernelized ridge regression objective function:
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}\frac{1}{n}\|K\alpha-y\|^{2}+\lambda\alpha^{T}K\alpha
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is usually just called 
\series bold
kernel ridge regression
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Ridge Regression Solutions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For 
\begin_inset Formula $\lambda>0$
\end_inset

, the 
\series bold
ridge regression solution
\series default
 is 
\begin_inset Formula 
\[
w^{*}=(X^{T}X+\lambda I)^{-1}X^{T}y
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
and the 
\series bold
kernel ridge regression solution
\series default
 is
\begin_inset Formula 
\begin{eqnarray*}
\alpha^{*} & = & (XX^{T}+\lambda I)^{-1}y\\
\pause & = & (K+\lambda I)^{-1}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
(Shown in homework.)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For ridge regression we're dealing with a 
\begin_inset Formula $d\times d$
\end_inset

 matrix.
\end_layout

\begin_layout Itemize
For kernel ridge regression we're dealing an 
\begin_inset Formula $n\times n$
\end_inset

 matix.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Predictions in terms of 
\begin_inset Formula $w^{*}$
\end_inset

:
\begin_inset Formula 
\[
\hat{f}(x)=x^{T}w^{*}\pause
\]

\end_inset


\end_layout

\begin_layout Itemize
Predictions in terms of 
\begin_inset Formula $\alpha^{*}$
\end_inset

:
\begin_inset Formula 
\[
\hat{f}(x)=k_{x}^{T}\alpha^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}x_{i}^{T}x\pause
\]

\end_inset


\end_layout

\begin_layout Itemize
For kernel ridge regression, need to access all training inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 to predict.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For SVM, we may not...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Kernel SVM
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized SVM (From Representer Theorem) 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM objective:
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}w^{T}x_{i}\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Plugging in 
\begin_inset Formula $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$
\end_inset

, we get
\begin_inset Formula 
\[
\min_{\alpha\in\reals^{n}}\frac{1}{2}\alpha^{T}K\alpha+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left(K\alpha\right)_{i}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Predictions with
\begin_inset Formula 
\[
\hat{f}(x)=x^{T}w^{*}\pause=\sum_{i=1}^{n}\alpha_{i}^{*}x_{i}^{T}x.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is one way to kernelize SVM...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized SVM (From Lagrangian Duality) 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernelized SVM from computing the Lagrangian Dual Problem:
\begin_inset Formula 
\begin{eqnarray*}
\max_{\alpha\in\reals^{n}} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $\alpha^{*}$
\end_inset

 is an optimal value, then
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}\qquad\text{and}\qquad\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}^{T}x.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that the prediction function is also kernelized.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Sparsity in the Data from Complementary Slackness
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernelized predictions given by
\begin_inset Formula 
\[
\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}^{T}x.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Seems to need all training inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 to make a prediction on a new 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
By a Lagrangian duality analysis (specifically from complementary slackness),
 we find 
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\hat{f}(x_{i})<1 & \implies & \alpha_{i}^{*}=\frac{c}{n}\\
y_{i}\hat{f}(x_{i})=1 & \implies & \alpha_{i}^{*}\in\left[0,\frac{c}{n}\right]\\
y_{i}\hat{f}(x_{i})>1 & \implies & \alpha_{i}^{*}=0
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So we can leave out any 
\begin_inset Formula $x_{i}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

on the good side of the margin
\begin_inset Quotes erd
\end_inset

 (
\begin_inset Formula $y_{i}\hat{f}(x_{i})>1$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x_{i}$
\end_inset

's that we must keep, because 
\begin_inset Formula $\alpha_{i}^{*}\neq0$
\end_inset

, are called 
\series bold
support vectors
\series default
.
\end_layout

\end_deeper
\begin_layout Section
Kernels
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{d}$
\end_inset

, with standard inner product
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Feature map
\begin_inset Formula 
\[
\psi(x)=x
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernel: 
\begin_inset Formula 
\[
k(x,x')=x^{T}x'
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature space: 
\begin_inset Formula $\ch=\reals^{D}$
\end_inset

, where 
\begin_inset Formula $D=d+{d \choose 2}\approx d^{2}/2$
\end_inset

.
\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\ensuremath{\psi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then for 
\begin_inset Formula $\forall x,x'\in\reals^{d}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
k(x,x') & = & \left\langle \psi(x),\psi(x')\right\rangle \\
\pause & = & \left\langle x,x'\right\rangle +\left\langle x,x'\right\rangle ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Computation for inner product with explicit mapping: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Computation for implicit kernel calculation: 
\begin_inset Formula $O(d)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Kernel function:
\begin_inset Formula 
\[
k(x,x')=\left(1+\left\langle x,x'\right\rangle \right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponds to a feature map with all monomials up to degree 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $M$
\end_inset

, computing the kernel has same computational cost
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cost of explicit inner product computation grows rapidly in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Section
The RBF Kernel
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) / Gaussian Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(x,x')=\exp\left(-\frac{\|x-x'\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinite dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Basis
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Output space: 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
RBF kernel 
\begin_inset Formula $k(w,x)=\exp\left(-\left(w-x\right)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Suppose we have 6 training examples: 
\begin_inset Formula $x_{i}\in\left\{ -6,-4,-3,0,2,4\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If representer theorem applies, then
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $f$
\end_inset

 is a linear combination of 6 basis functions of form 
\begin_inset Formula $k(x_{i},\cdot)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Basis functions
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Predictions of the form 
\begin_inset Formula $f(x)=\sum_{i=1}^{6}\alpha_{i}k(x_{i},x)$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/kernels/kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When kernelizing with RBF kernel, prediction functions always look this
 way.
\end_layout

\begin_layout Itemize
(Whether we get 
\begin_inset Formula $w$
\end_inset

 from SVM, ridge regression, etc...)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Feature Space: The Sequence Space 
\begin_inset Formula $\ell_{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To work with infinite dimensional feature vectors, we need a space with
 certain properties.
\end_layout

\begin_deeper
\begin_layout Itemize
an inner product
\end_layout

\begin_layout Itemize
a norm related to the inner product 
\end_layout

\begin_layout Itemize
projection theorem: 
\begin_inset Formula $x=x_{\perp}+x_{\|}$
\end_inset

 where 
\begin_inset Formula $x_{\|}\in S=\linspan(w_{1},\ldots,w_{n})$
\end_inset

 and 
\begin_inset Formula $\left\langle x_{\perp},s\right\rangle =0\quad\forall s\in S$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize
Basically, we need a Hilbert space.
\end_layout

\begin_layout Definition
\begin_inset Formula $\ell_{2}$
\end_inset

 is the space of all real-valued sequences: 
\begin_inset Formula $\left(x_{0},x_{1},x_{2},x_{3},\ldots\right)$
\end_inset

 with 
\begin_inset Formula $\sum_{i=0}^{\infty}x_{i}^{2}<\infty$
\end_inset

.
 
\end_layout

\begin_layout Theorem
With the the inner product 
\begin_inset Formula $\left\langle x,x'\right\rangle =\sum_{i=0}^{\infty}x_{i}x'_{i}$
\end_inset

, 
\begin_inset Formula $\ell_{2}$
\end_inset

 is a 
\series bold
Hilbert space
\series default
.
\end_layout

\begin_layout Theorem

\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
We know the inner product between 
\begin_inset Formula $x,x'\in\ell_{2}$
\end_inset

 is well-defined, as follows.
 First, note the following: For any numbers 
\begin_inset Formula $a,b\in\reals$
\end_inset

, we have 
\begin_inset Formula $\left|ab\right|\le\left|a\right|\left|b\right|\le\left(a^{2}+b^{2}\right)/2$
\end_inset

, since: 
\begin_inset Formula 
\begin{eqnarray*}
\left(\left|a\right|-\left|b\right|\right)^{2} & \ge & 0\\
\implies a^{2}+b^{2} & \ge & 2\left|a\right|\left|b\right|
\end{eqnarray*}

\end_inset

 Then let 
\begin_inset Formula $M=\sum_{i=0}^{\infty}x_{i}^{2}$
\end_inset

 and 
\begin_inset Formula $M'=\sum_{i=0}^{\infty}\left(x_{i}'\right)^{2}$
\end_inset

.
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=0}^{n}\left|x_{i}x_{i}'\right| & \le & \sum_{i=0}^{\infty}\left|x_{i}x_{i}'\right|\\
 & \le & \frac{1}{2}\left[\sum_{i=0}^{\infty}\left(x_{i}^{2}\right)+\sum_{i=0}^{\infty}\left(x_{i}'\right)^{2}\right]\le\left(M+M'\right)/2
\end{eqnarray*}

\end_inset

Thus 
\begin_inset Formula $\sum_{i=0}^{\infty}x_{i}x_{i}'$
\end_inset

 converges (since it is an absolutely convergent series).
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Infinite Dimensional Feature Vector for RBF
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider RBF kernel (1-dim): 
\begin_inset Formula $k(x,x')=\exp\left(-\left(x-x'\right)^{2}/2\right)$
\end_inset


\end_layout

\begin_layout Itemize
We claim that 
\begin_inset Formula $\psi:\reals\to\ell_{2}$
\end_inset

, defined by 
\begin_inset Formula 
\[
\left[\psi(x)\right]_{j}=\frac{1}{\sqrt{j!}}e^{-x^{2}/2}x^{j}
\]

\end_inset

 gives the 
\series bold

\begin_inset Quotes eld
\end_inset

infinite-dimensional feature vector
\begin_inset Quotes erd
\end_inset

 corresponding to RBF kernel
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Is this mapping even well-defined? Is 
\begin_inset Formula $\psi(x)$
\end_inset

 even an element of 
\begin_inset Formula $\ell_{2}$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Yes: 
\begin_inset Formula 
\[
\sum_{j=0}^{\infty}\frac{1}{j!}e^{-x^{2}}x^{2j}=e^{-x^{2}}\sum_{j=0}^{\infty}\frac{\left(x^{2}\right)^{j}}{j!}=1<\infty
\]

\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Infinite Dimensional Feature Vector for RBF
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Does feature vector 
\begin_inset Formula $\left[\psi(x)\right]_{n}=\frac{1}{\sqrt{j!}}e^{-x^{2}/2}x^{j}$
\end_inset

 actually correspond to the RBF kernel?
\end_layout

\begin_layout Itemize
Yes! Proof:
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \psi(x),\psi(x')\right\rangle  & = & \sum_{j=0}^{\infty}\frac{1}{j!}e^{-\left(x^{2}+\left(x'\right)^{2}\right)/2}x^{j}\left(x'\right)^{j}\\
 & = & e^{-\left(x^{2}+\left(x'\right)^{2}\right)/2}\sum_{j=0}^{\infty}\frac{\left(xx'\right)^{j}}{j!}\\
 & = & \exp\left(-\left[x^{2}+\left(x'\right)^{2}\right]/2\right)\exp\left(xx'\right)\\
 & = & \exp\left(-\left[(x-x')^{2}/2\right]\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
QED 
\end_layout

\end_deeper
\begin_layout Section
When is 
\begin_inset Formula $k(x,x')$
\end_inset

 a kernel function? (Mercer's Theorem)
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to Get Kernels?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Explicitly construct 
\begin_inset Formula $\psi(x):\cx\to\reals^{d}$
\end_inset

 and define 
\begin_inset Formula $k(x,x')=\psi(x)^{T}\psi(x')$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Directly define the kernel function 
\begin_inset Formula $k(x,x')$
\end_inset

, and verify it corresponds to 
\begin_inset Formula $\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

 for some 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
There are many theorems to help us with the second approach
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Matrices
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A real, symmetric matrix 
\begin_inset Formula $M\in\reals^{n\times n}$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any 
\begin_inset Formula $x\in\reals^{n}$
\end_inset

, 
\begin_inset Formula 
\[
x^{T}Mx\ge0.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Theorem
The following conditions are each necessary and sufficient for a symmetric
 matrix 
\begin_inset Formula $M$
\end_inset

 to be positive semidefinite:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $M$
\end_inset

 has can be factorized as 
\begin_inset Formula $M=R^{T}R$
\end_inset

, for some matrix 
\begin_inset Formula $R$
\end_inset

.
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $M$
\end_inset

 are greater than or equal to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Positive Semidefinite Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A symmetric kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is 
\series bold
positive semidefinite (psd)
\series default
 if for any finite set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} \in\cx$
\end_inset

, the kernel matrix on this set 
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}
\]

\end_inset

is a positive semidefinite matrix.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mercer's Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
A symmetric function 
\begin_inset Formula $k(x,x')$
\end_inset

 can be expressed as an inner product
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\psi$
\end_inset

 if and only if 
\begin_inset Formula $k(x,x')$
\end_inset

 is 
\series bold
positive semidefinite.
\begin_inset Note Note
status open

\begin_layout Theorem

\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Proof
[Sketch] Suppose 
\begin_inset Formula $k(w,x)$
\end_inset

 is psd.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generating New Kernels from Old
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k,k_{1},k_{2}:\cx\times\cx\to\reals$
\end_inset

 are psd kernels.
 Then so are the following:
\begin_inset Formula 
\begin{eqnarray*}
k_{\mbox{new}}(x,x') & = & k_{1}(x,x')+k_{2}(x,x')\\
k_{\mbox{new}}(x,x') & = & \alpha k(x,x')\\
k_{\mbox{new}}(x,x') & = & f(x)f(x')\mbox{ for any function \ensuremath{f(\cdot)}}\\
k_{\mbox{new}}(x,x') & = & k_{1}(x,x')k_{2}(x,x')
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
See Appendix for details.
\end_layout

\begin_layout Itemize
Lots more theorems to help you construct new kernels from old...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Details on New Kernels from Old [Optional]
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Additive Closure
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(x,x')+k_{2}(x,x')
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Concatenate the feature vectors to get 
\begin_inset Formula 
\[
\phi(x)=\left(\phi_{1}(x),\phi_{2}(x)\right).
\]

\end_inset

Then 
\begin_inset Formula $\phi$
\end_inset

 is a feature map for 
\begin_inset Formula $k_{1}+k_{2}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Positive Scaling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k$
\end_inset

 is a psd kernel with feature maps 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\begin_layout Itemize
Then for any 
\begin_inset Formula $\alpha>0$
\end_inset

, 
\begin_inset Formula 
\[
\alpha k
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Note that 
\begin_inset Formula 
\[
\phi(x)=\sqrt{\alpha}\phi(x)
\]

\end_inset

 is a feature map for 
\begin_inset Formula $\alpha k$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Scalar Function Gives a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For any function 
\begin_inset Formula $f(x)$
\end_inset

, 
\begin_inset Formula 
\[
k(x,x')=f(x)f(x')
\]

\end_inset

is a kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Let 
\begin_inset Formula $f(x)$
\end_inset

 be the feature mapping.
 (It maps into a 1-dimensional feature space.)
\begin_inset Formula 
\[
\left\langle f(x),f(x')\right\rangle =f(x)f(x')=k(x,x').
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose 
\begin_inset Formula $k_{1}$
\end_inset

 and 
\begin_inset Formula $k_{2}$
\end_inset

 are psd kernels with feature maps 
\begin_inset Formula $\phi_{1}$
\end_inset

 and 
\begin_inset Formula $\phi_{2}$
\end_inset

, respectively.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula 
\[
k_{1}(x,x')k_{2}(x,x')
\]

\end_inset

is a psd kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Proof: Take the outer product of the feature vectors: 
\begin_inset Formula 
\[
\phi(x)=\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}.
\]

\end_inset

Note that 
\begin_inset Formula $\phi(x)$
\end_inset

 is a matrix.
\end_layout

\begin_layout Itemize
Continued...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Closure under Hadamard Products
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Then
\begin_inset Formula 
\begin{eqnarray*}
\left\langle \phi(x),\phi(x')\right\rangle  & = & \sum_{i,j}\phi(x)\phi(x')\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\left[\phi_{2}(x)\right]^{T}\right]_{ij}\left[\phi_{1}(x')\left[\phi_{2}(x')\right]^{T}\right]_{ij}\\
 & = & \sum_{i,j}\left[\phi_{1}(x)\right]_{i}\left[\phi_{2}(x)\right]_{j}\left[\phi_{1}(x')\right]_{i}\left[\phi_{2}(x')\right]_{j}\\
 & = & \left(\sum_{i}\left[\phi_{1}(x)\right]_{i}\left[\phi_{1}(x')\right]_{i}\right)\left(\sum_{j}\left[\phi_{2}(x)\right]_{j}\left[\phi_{2}(x')\right]_{j}\right)\\
 & = & k_{1}(x,x')k_{2}(x,x')
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\end_body
\end_document
